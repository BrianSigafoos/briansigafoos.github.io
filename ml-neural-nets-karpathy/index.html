<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><title>Learning ML neural nets with Andrej Karpathy - Brian Sigafoos</title><link rel=stylesheet href="/css/main.min.b2e9e37f1f1f29069ea31873433a546c6c9bc06754625faf1f8b64b845495485.css" integrity="sha256-sunjfx8fKQaeoxhzQzpUbGybwGdUYl+vH4tkuEVJVIU="><script src=/main.js></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-42740116-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<link rel="shortcut icon" href=/favicon.png?20190809><link rel=apple-touch-icon href=/apple-touch-icon.png?20190809></head><body data-controller=color-scheme class="text-color bg-bright antialiased leading-normal"><div class="border-b border-gray py-4 w-full"><div class="container max-w-4xl"><div class="flex items-center justify-between font-mono"><a href=https://briansigafoos.com/ class="text-primary font-extrabold">Brian Sigafoos</a><div class="flex items-center"><a href=/about class="text-color font-extrabold block">About me</a><div class="flex ml-4"><button type=button data-action=color-scheme#toggleScheme aria-label="Toggle dark mode" title="Toggle dark mode" class="py-1 px-2 cursor-pointer text-color"><svg class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg></button>
<button type=button data-action=color-scheme#toggleColor aria-label="Change primary color" title="Change primary color" class="py-1 px-2 cursor-pointer text-primary"><svg class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zm0 0h12a2 2 0 002-2v-4a2 2 0 00-2-2h-2.343M11 7.343l1.657-1.657a2 2 0 012.828.0l2.829 2.829a2 2 0 010 2.828l-8.486 8.485M7 17h.01"/></svg></button></div></div></div></div></div><div class="container my-12 md:my-16 !max-w-4xl prose md:prose-lg lg:prose-xl prose-custom"><h1>Learning ML neural nets with Andrej Karpathy</h1><p class="mt-4 text-lg md:text-xl lg:text-2xl text-muted">Notes from following Karpathy&rsquo;s machine learning videos: Neural Networks: Zero to Hero</p><div class="font-mono text-sm mt-3 space-x-4"><span class=text-muted>Nov 23, 2022</span>
<span class=text-muted>5 min read</span></div><div class="border-t border-gray my-8 pt-4"><h2 id=intro>Intro</h2><p>These are my ongoing notes from learning foundational Machine Learning (ML) concepts, as taught by Andrej Karpathy.
Andrej is the former Director of AI at Tesla, and an excellent teacher.
He demystifies complex ML topics like gradient descent through simple examples. When following these video you can (and should) easily recreate everything he does on your local machine.</p><p>YouTube playlist: <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a></p><p>YouTub videos:</p><ul><li><ol><li><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to neural networks and backpropagation: building micrograd</a></li></ol></li><li><ol start=2><li><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to language modeling: building makemore</a></li></ol></li><li>&mldr;</li></ul><p>Github repos:</p><ul><li><a href=https://github.com/karpathy/micrograd>micrograd</a> - A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API</li><li><a href=https://github.com/karpathy/makemore>makemore</a> - An autoregressive character-level language model for making more things</li></ul><h3 id=whats-a-neural-network>What&rsquo;s a neural network?</h3><p>Here&rsquo;s how Wikipedia defines a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>:</p><blockquote><p>A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes.</p></blockquote><blockquote><p>Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems.</p></blockquote><blockquote><p>The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination.</p></blockquote><blockquote><p>Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be âˆ’1 and 1.</p></blockquote><h2 id=setup>Setup</h2><p>First, install Python if you haven&rsquo;t yet.</p><p>Use <a href=https://github.com/pyenv/pyenv-installer>pyenv</a>, similar to rbenv, to easily manage versions.
Note: I recommend an older Python version like 3.9 because PyTorch, a key dependency later on, doesn&rsquo;t always work with the latest version it seems:</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pyenv install -l  <span style=color:#999;font-style:italic># list remote</span>
</span></span><span style=display:flex><span>pyenv install 3.9 <span style=color:#999;font-style:italic># install</span>
</span></span><span style=display:flex><span>pyenv <span style=color:#24909d>local</span> 3.9   <span style=color:#999;font-style:italic># use it locally</span>
</span></span><span style=display:flex><span>pyenv versions    <span style=color:#999;font-style:italic># list local</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pyenv init
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># then update ~/.zshrc with the output to be able to use correct version of</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># python via `python` and `pip` commands without calling `python3`, `pip3`, etc</span>
</span></span></code></pre></div><p>Then, open VS Code and install its <a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python">Python extension</a></p><p>Then <code>git clone</code> Andrej&rsquo;s repositories locally:</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>git clone git@github.com:karpathy/micrograd.git
</span></span><span style=display:flex><span>git clone git@github.com:karpathy/makemore.git
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Open in VS Code</span>
</span></span><span style=display:flex><span>code micrograd
</span></span></code></pre></div><p>Open a Jupyter Notebook <code>.ipynb</code> file in <code>micrograd</code> and select the <code>pyenv</code> version you installed, plus install any VSCode recommended extensions for Jupyter Notebooks.</p><p>Create a new file called <code>youtube1.ipynb</code> or something similar so you can run same commands that Andrej does during his videos.</p><p>Now you&rsquo;re all set to dive into the videos.</p><h2 id=1-micrograd>1. Micrograd</h2><p>Watch the YouTube video: <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to neural networks and backpropagation: building micrograd</a>.</p><p>And follow along locally in your own <code>.ipynb</code> file. I strongly recommend doing this yourself, typing out everything that Andrej does, and running it all locally. This &ldquo;practice&rdquo; helps me learn the content better and builds confidence that it can all be recreated locally.</p><p>Just for reference (create your own!), here are examples of local notebooks:</p><ul><li>My notebook from the <a href=https://github.com/BrianSigafoos/micrograd/blob/master/micrograd_youtube1.ipynb>1st part of the micrograd video</a>.</li><li>Two more complete <a href=https://github.com/Anri-Lombard/micrograd/tree/main/Lectures>notebooks for both parts of the micrograd video</a> by Github user @Anri-Lombard.</li></ul><p>Everything needed to understand a neural network is in <a href=https://github.com/karpathy/micrograd>micrograd</a>. Everything else is just efficiency.
There are only 150 lines of code in <code>micrograd/engine.py</code> and <code>micrograd/nn.py</code>.</p><p>&ldquo;Back propagation is recursive application of chain rule, backwards through the computation graph.&rdquo;</p><p>If can write local gradients and can do backward propagation of gradients, then it doesn&rsquo;t matter if it&rsquo;s a compound function or separate functions (equal to compound function), the result will be the same.</p><p><code>micrograd</code> is for scalar values (1.0, etc)
PyTorch is for a tensor, an n-dimensional array of scalars</p><p>Gradient descent is the iteration of:</p><ul><li>forward pass</li><li>backward pass</li><li>update</li></ul><p>The neural net improved its predictions with each iteration.</p><p>Most common neural net mistakes from <a href=https://twitter.com/karpathy/status/1013244313327681536>@karpathy&rsquo;s tweet</a>:</p><ol><li>you didn&rsquo;t try to overfit a single batch first.</li><li>you forgot to toggle train/eval mode for the net.</li><li>you forgot to .zero_grad() (in pytorch) before .backward().</li><li>you passed softmaxed outputs to a loss that expects raw logits.</li><li>you didn&rsquo;t use bias=False for your Linear/Conv2d layer when using BatchNorm, or conversely forget to include it for the output layer. This one won&rsquo;t make you silently fail, but they are spurious parameters</li><li>thinking view() and permute() are the same thing (& incorrectly using view)</li></ol><blockquote class=twitter-tweet><p lang=en dir=ltr>most common neural net mistakes: 1) you didn't try to overfit a single batch first. 2) you forgot to toggle train/eval mode for the net. 3) you forgot to .zero_grad() (in pytorch) before .backward(). 4) you passed softmaxed outputs to a loss that expects raw logits. ; others? :)</p>&mdash; Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/1013244313327681536?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><h2 id=references>References</h2><p>YouTube playlist: <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a></p><p>YouTub videos:</p><ul><li><ol><li><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to neural networks and backpropagation: building micrograd</a></li></ol></li><li><ol start=2><li><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to language modeling: building makemore</a></li></ol></li><li>&mldr;</li></ul><p>Github repos:</p><ul><li><a href=https://github.com/karpathy/micrograd>micrograd</a> - A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API</li><li><a href=https://github.com/karpathy/makemore>makemore</a> - An autoregressive character-level language model for making more things</li></ul><h3 id=lecture-notes-by-others>Lecture notes by others</h3><ul><li><a href=https://github.com/Anri-Lombard/micrograd>https://github.com/Anri-Lombard/micrograd</a></li><li><a href=https://github.com/Anri-Lombard/makemore>https://github.com/Anri-Lombard/makemore</a></li></ul><div class="border-t border-b border-gray text-muted my-8 py-4">Read more posts like this in the
<a href=/software-engineering-toolbox>Software Engineering Toolbox</a> collection.</div></div><div class=my-8><a href=https://briansigafoos.com/ class="text-base inline-block mb-1 py-2 px-6 cursor-pointer font-mono font-bold text-primary border border-solid border-primary bg-bright no-underline">Visit homepage</a></div><div class=my-8><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//briansigafoos.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class="border-t border-gray py-4 my-16 w-full text-sm text-faint"><div class="container max-w-4xl text-center font-mono"><div class="sm:flex items-center justify-between"><span>Made with
<a href=https://gohugo.io rel=nofollow target=hugo class=underline>Hugo</a> &
        <a href=https://tailwindcss.com rel=nofollow target=tw class=underline>TailwindCSS</a>
Â·
<a href=https://github.com/BrianSigafoos/hugo_site rel=nofollow target=source class=underline>Source code</a></span>
<span class="block sm:inline mt-2 sm:mt-0">By <a href=/about class="text-primary font-black">Brian Sigafoos</a></span></div></div></div></body></html>