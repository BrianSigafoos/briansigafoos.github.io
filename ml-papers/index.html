<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><title>ML papers - Brian Sigafoos</title><link rel=stylesheet href="/css/main.min.36f0931af260e3bf93da27c4eafc4fb654b04306822a772442a8b4241cd5eaad.css" integrity="sha256-NvCTGvJg47+T2ifE6vxPtlSwQwaCKnckQqi0JBzV6q0="><script src=/main.js></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-42740116-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<link rel="shortcut icon" href=/favicon.png?20190809><link rel=apple-touch-icon href=/apple-touch-icon.png?20190809></head><body data-controller=color-scheme class="text-color bg-bright antialiased leading-normal"><div class="border-b border-gray py-4 w-full"><div class="container max-w-4xl"><div class="flex items-center justify-between font-mono"><a href=https://briansigafoos.com/ class="text-primary font-extrabold">Brian Sigafoos</a><div class="flex items-center"><a href=/about class="text-color font-extrabold block">About me</a><div class="flex ml-4"><button type=button data-action=color-scheme#toggleScheme aria-label="Toggle dark mode" title="Toggle dark mode" class="py-1 px-2 cursor-pointer text-color"><svg class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg></button>
<button type=button data-action=color-scheme#toggleColor aria-label="Change primary color" title="Change primary color" class="py-1 px-2 cursor-pointer text-primary"><svg class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zm0 0h12a2 2 0 002-2v-4a2 2 0 00-2-2h-2.343M11 7.343l1.657-1.657a2 2 0 012.828.0l2.829 2.829a2 2 0 010 2.828l-8.486 8.485M7 17h.01"/></svg></button></div></div></div></div></div><div class="container my-12 md:my-16 !max-w-4xl prose md:prose-lg lg:prose-xl prose-custom"><h1>ML papers</h1><p class="mt-4 text-lg md:text-xl lg:text-2xl text-muted">A collection of interesting and important machine learning papers</p><div class="font-mono text-sm mt-3 space-x-4"><span class=text-muted>Jan 27, 2023</span>
<span class=text-muted>7 min read</span></div><div class="border-t border-gray my-8 pt-4"><h2 id=reading-list>Reading List</h2><p>New and interesting papers</p><ul><li>2023 - <a href=https://arxiv.org/abs/2302.01329>Dreamix: Video Diffusion Models are General Video Editors</a> - <a href=https://dreamix-video-editing.github.io/>project page</a></li><li>2023 - <a href=https://arxiv.org/abs/2301.11093>simple diffusion: End-to-end diffusion for high resolution images</a></li><li>2023 - <a href=https://arxiv.org/abs/2301.00774>SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot</a></li><li>2023 - <a href=https://arxiv.org/abs/2301.01162>Language Models are Drummers: Drum Composition with Natural Language Pre-Training</a></li><li>2021 - <a href=https://arxiv.org/abs/2105.14103>An Attention Free Transformer</a><ul><li>Inspired by this is <a href=https://github.com/BlinkDL/RWKV-LM>RWKV is a RNN with transformer-level LLM performance</a></li></ul></li></ul><h2 id=stable-diffusion>Stable Diffusion</h2><ul><li>2022 - <a href=https://arxiv.org/abs/2210.09276>Imagic: Text-Based Real Image Editing with Diffusion Models</a> - <a href=https://imagic-editing.github.io>project page</a></li><li>2022 - <a href=https://arxiv.org/abs/2208.12242>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a> - <a href=https://dreambooth.github.io/>project page</a></li><li>2021 - <a href=https://arxiv.org/abs/2112.10752>High-Resolution Image Synthesis with Latent Diffusion Models</a> - introduction of Stable Diffusion</li><li>2021 - <a href=https://arxiv.org/abs/2106.09685>LoRA: Low-Rank Adaptation of Large Language Models</a><ul><li>2023 - <a href=https://huggingface.co/blog/lora>Using LoRA for Efficient Stable Diffusion Fine-Tuning</a></li></ul></li></ul><h2 id=important-papers>Important papers</h2><p>Most are seminal, introducing something new, and a few are reviews that help summarize.</p><ul><li><p>2020 - <a href=https://arxiv.org/abs/2010.11929>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a> - ViT (Vision Transformer)</p><ul><li><a href=https://en.wikipedia.org/wiki/Vision_transformer>Wikipedia: Vision transformer</a>: Transformers measure the relationships between pairs of input tokens (words in the case of text strings), termed attention. The cost is quadratic in the number of tokens. For images, the basic unit of analysis is the pixel. However, computing relationships for every pixel pair in a typical image is prohibitive in terms of memory and computation. Instead, ViT computes relationships among pixels in various small sections of the image (e.g., 16x16 pixels), at a drastically reduced cost. The sections (with positional embeddings) are placed in a sequence. The embeddings are learnable vectors. Each section is arranged into a linear sequence and multiplied by the embedding matrix. The result, with the position embedding is fed to the transformer.</li><li>ChatGPT summary: .. introduces a new image recognition model called ViT (Vision Transformers). Unlike traditional image recognition models that use convolutional neural networks (CNNs), ViT applies the transformer architecture from NLP to image recognition. ViT uses a patch-based approach, where an image is divided into non-overlapping patches, and each patch is fed into a transformer encoder to obtain a set of features. The features are then used to classify the image. The authors show that ViT outperforms previous state-of-the-art models on several benchmark datasets, demonstrating the effectiveness of the transformer architecture in image recognition. The paper also highlights the scalability of ViT, as it can handle high-resolution images and achieve improved accuracy with larger model sizes.</li></ul></li><li><p>2020 - <a href=https://arxiv.org/abs/2003.05991>Autoencoders</a> - Summary paper</p></li><li><p>2020 - <a href=https://arxiv.org/abs/2005.14165>Language Models are Few-Shot Learners</a> - &ldquo;GPT-3&rdquo;</p><ul><li><a href=https://en.wikipedia.org/wiki/GPT-3>Wikipedia: GPT-3</a>: The architecture is a decoder-only transformer network with a 2048-token-long context and then-unprecedented size of 175 billion parameters, requiring 800GB to store. The model was trained using generative pre-training; it is trained to predict what the next token is based on previous tokens. The model demonstrated strong zero-shot and few-shot learning on many tasks. The authors described how language understanding performances in natural language processing (NLP) were improved in GPT-n through a process of &ldquo;generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.&rdquo; This eliminated the need for human supervision and for time-intensive hand-labeling.</li><li>ChatGPT summary: &mldr; investigates the ability of large language models such as GPT-3 to perform well on a wide range of tasks with only a small amount of fine-tuning or task-specific training data. The authors show that fine-tuning these models on a small dataset can lead to performance that is comparable or even superior to models trained from scratch on the same dataset, demonstrating the effectiveness of pre-training in NLP. The study also highlights the robustness of the models, as they can perform well on a wide range of tasks and domains, even when the tasks are very different from the pre-training data. The paper concludes that large language models are powerful few-shot learners, capable of adapting to new tasks with minimal fine-tuning, and has significant implications for NLP and AI in general.</li></ul></li><li><p>2019 - <a href=https://arxiv.org/abs/1909.09586>Understanding LSTM &ndash; a tutorial into Long Short-Term Memory Recurrent Neural Networks</a> - LSTM</p></li><li><p>2018 - <a href=https://arxiv.org/abs/1810.04805>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p><ul><li><a href=https://en.wikipedia.org/wiki/BERT_(language_model)>Wikipedia: BERT</a>: &ldquo;Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. &mldr; The original English-language BERT has two models: (1) the BERTBASE: 12 encoders with 12 bidirectional self-attention heads, and (2) the BERTLARGE: 24 encoders with 16 bidirectional self-attention heads. Both models are pre-trained from unlabeled data extracted from the BooksCorpus[4] with 800M words and English Wikipedia with 2,500M words.&rdquo;</li></ul></li><li><p>2017 - <a href=https://arxiv.org/abs/1711.05101>Decoupled Weight Decay Regularization</a> - &ldquo;AdamW&rdquo;</p></li><li><p>2017 - <a href=https://arxiv.org/abs/1711.00937>Neural Discrete Representation Learning</a> - &ldquo;VQ-VAE&rdquo;, vector autoencoders</p></li><li><p>2017 - <a href=https://arxiv.org/abs/1706.03762>Attention Is All You Need</a> - introduces the transformer architecture</p><ul><li><a href=https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)>Wikipedia: Transformer</a>: A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP) and computer vision (CV).</li><li>Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times."</li><li>Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly the model of choice for NLP problems, replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.</li><li>ChatGPT summary: &mldr; presents a novel Transformer architecture for neural machine translation (NMT) that relies solely on self-attention mechanisms and dispenses with recurrence and convolutions. The Transformer model consists of an encoder and a decoder, both of which are composed of a stack of self-attention and feed-forward layers. The self-attention mechanism allows each position in the sequence to attend to all other positions, enabling the model to capture long-range dependencies and contextual information.</li></ul></li><li><p>2016 - <a href=https://arxiv.org/abs/1609.03499>WaveNet: A Generative Model for Raw Audio</a></p></li><li><p>2016 - <a href=https://arxiv.org/abs/1607.06450>Layer Normalization</a></p></li><li><p>2015 - <a href=https://arxiv.org/abs/1512.03385>Deep Residual Learning for Image Recognition</a> - &ldquo;ResNet&rdquo;, introduces &ldquo;residual blocks&rdquo; that transform data, then a skip connection from the previous features</p></li><li><p>2015 - <a href=https://arxiv.org/abs/1502.03167>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> - &ldquo;batch norm&rdquo;</p></li><li><p>2014 - <a href=https://arxiv.org/abs/1412.6980>Adam: A Method for Stochastic Optimization</a> - &ldquo;Adam&rdquo;</p></li><li><p>2014 - <a href=https://dl.acm.org/doi/abs/10.5555/2627435.2670313>Dropout: a simple way to prevent neural networks from overfitting</a></p></li><li><p>2014 - <a href=https://arxiv.org/abs/1406.1078v3>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a> - &ldquo;GRU&rdquo;, a Gated Recurrent Unit</p><ul><li>GRU is a type of recurrent neural network (RNN). It is similar to an LSTM, but only has two gates - a reset gate and an update gate - and notably lacks an output gate. Fewer parameters means GRUs are generally easier/faster to train than their LSTM counterparts</li></ul></li><li><p>2014 - <a href=https://arxiv.org/abs/1406.2661>Generative Adversarial Networks</a> - &ldquo;GAN&rdquo;</p><ul><li>ChatGPT summary: &mldr; presents a novel architecture for generative models in machine learning. GANs consist of two neural networks, a generator and a discriminator, that are trained simultaneously in a game-theoretic setup where the generator aims to produce samples that are indistinguishable from real data, while the discriminator tries to correctly classify whether each sample is real or fake. The training process continues until the generator produces high-quality samples that the discriminator is unable to reliably distinguish from real data. The authors show that GANs are capable of generating diverse and highly-realistic samples, making them a powerful tool for various applications such as image synthesis, data augmentation, and unsupervised representation learning.</li></ul></li></ul><h2 id=resources>Resources</h2><ul><li><a href="https://www.youtube.com/watch?v=ZXuK6IRJlnk">Video walkthrough: Progressive Distillation for Fast Sampling of Diffusion Models</a></li></ul><div class="border-t border-b border-gray text-muted my-8 py-4">Read more posts like this in the
<a href=/software-engineering-toolbox>Software Engineering Toolbox</a> collection.</div></div><div class=my-8><a href=https://briansigafoos.com/ class="text-base inline-block mb-1 py-2 px-6 cursor-pointer font-mono font-bold text-primary border border-solid border-primary bg-bright no-underline">Visit homepage</a></div><div class=my-8><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//briansigafoos.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class="border-t border-gray py-4 my-16 w-full text-sm text-faint"><div class="container max-w-4xl text-center font-mono"><div class="sm:flex items-center justify-between"><span>Made with
<a href=https://gohugo.io rel=nofollow target=hugo class=underline>Hugo</a> &
        <a href=https://tailwindcss.com rel=nofollow target=tw class=underline>TailwindCSS</a>
·
<a href=https://github.com/BrianSigafoos/hugo_site rel=nofollow target=source class=underline>Source code</a></span>
<span class="block sm:inline mt-2 sm:mt-0">By <a href=/about class="text-primary font-black">Brian Sigafoos</a></span></div></div></div></body></html>