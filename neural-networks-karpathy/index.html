<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><title>Learn Neural Networks with Andrej Karpathy - Brian Sigafoos</title><link rel=stylesheet href="/css/main.min.36f0931af260e3bf93da27c4eafc4fb654b04306822a772442a8b4241cd5eaad.css" integrity="sha256-NvCTGvJg47+T2ifE6vxPtlSwQwaCKnckQqi0JBzV6q0="><script src=/main.js></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-42740116-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<link rel="shortcut icon" href=/favicon.png?20190809><link rel=apple-touch-icon href=/apple-touch-icon.png?20190809></head><body data-controller=color-scheme class="text-color bg-bright antialiased leading-normal"><div class="border-b border-gray py-4 w-full"><div class="container max-w-4xl"><div class="flex items-center justify-between font-mono"><a href=https://briansigafoos.com/ class="text-primary font-extrabold">Brian Sigafoos</a><div class="flex items-center"><a href=/about class="text-color font-extrabold block">About me</a><div class="flex ml-4"><button type=button data-action=color-scheme#toggleScheme aria-label="Toggle dark mode" title="Toggle dark mode" class="py-1 px-2 cursor-pointer text-color"><svg class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg></button>
<button type=button data-action=color-scheme#toggleColor aria-label="Change primary color" title="Change primary color" class="py-1 px-2 cursor-pointer text-primary"><svg class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zm0 0h12a2 2 0 002-2v-4a2 2 0 00-2-2h-2.343M11 7.343l1.657-1.657a2 2 0 012.828.0l2.829 2.829a2 2 0 010 2.828l-8.486 8.485M7 17h.01"/></svg></button></div></div></div></div></div><div class="container my-12 md:my-16 !max-w-4xl prose md:prose-lg lg:prose-xl prose-custom"><h1>Learn Neural Networks with Andrej Karpathy</h1><p class="mt-4 text-lg md:text-xl lg:text-2xl text-muted">Notes from Karpathy&rsquo;s machine learning lectures - Neural Networks: Zero to Hero</p><div class="font-mono text-sm mt-3 space-x-4"><span class=text-muted>Nov 22, 2022</span>
<span class=text-muted>21 min read</span></div><div class="border-t border-gray my-8 pt-4"><h2 id=intro>Intro</h2><p>These are my notes from the <a href=https://twitter.com/Karpathy>Andrej Karpathy</a> lecture series: <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a>. Andrej is the former Director of AI at Tesla and an excellent teacher. He demystifies complex ML topics like gradient descent through simple examples. When following these videos, I recommend recreating everything Andrej covers on your local machine. Typing out the example code and running it locally helps practice and build confidence that from simple building blocks we can build up powerful models.</p><p>YouTube playlist: <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a></p><p>YouTube videos:</p><ol><li><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to neural networks and backpropagation: building micrograd</a></li><li><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to language modeling: building makemore</a></li><li><a href="https://www.youtube.com/watch?v=TCH_1BHY58I&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Building makemore Part 2: MLP</a></li><li><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Building makemore Part 3: Activations & Gradients, BatchNorm</a></li><li><a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Building makemore Part 4: Becoming a Backprop Ninja</a></li><li><a href="https://www.youtube.com/watch?v=t3YJ5hKiMQ0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Building makemore Part 5: Building a WaveNet</a></li><li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let&rsquo;s build GPT: from scratch, in code, spelled out</a></li></ol><p>Github repos:</p><ul><li><a href=https://github.com/karpathy/micrograd>micrograd</a> - A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API</li><li><a href=https://github.com/karpathy/makemore>makemore</a> - An autoregressive character-level language model for making more things</li><li><a href=https://github.com/karpathy/nn-zero-to-hero>notebooks: nn-zero-to-hero</a> - Lecture notebooks to run locally</li><li><a href=https://github.com/karpathy/nanoGPT>nanoGPT</a> - Lecture 7 and <a href=https://github.com/karpathy/ng-video-lecture>lecture 7 repo</a></li></ul><h3 id=whats-a-neural-network>What&rsquo;s a neural network?</h3><p>Here&rsquo;s how Wikipedia defines a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>:</p><blockquote><p>A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes.</p></blockquote><blockquote><p>Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems.</p></blockquote><blockquote><p>The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination.</p></blockquote><blockquote><p>Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be âˆ’1 and 1.</p></blockquote><h2 id=setup>Setup</h2><p>First, install Python if you haven&rsquo;t yet.</p><p>Use <a href=https://github.com/pyenv/pyenv-installer>pyenv</a>, similar to rbenv, to easily manage versions.
Note: I recommend an older Python version like 3.9 because PyTorch, a key dependency later on, doesn&rsquo;t always work with the latest version it seems:</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pyenv install -l  <span style=color:#999;font-style:italic># list remote</span>
</span></span><span style=display:flex><span>pyenv install 3.9 <span style=color:#999;font-style:italic># install</span>
</span></span><span style=display:flex><span>pyenv <span style=color:#24909d>local</span> 3.9   <span style=color:#999;font-style:italic># use it locally</span>
</span></span><span style=display:flex><span>pyenv versions    <span style=color:#999;font-style:italic># list local</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pyenv init
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># then update ~/.zshrc with the output to be able to use correct version of</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># python via `python` and `pip` commands without calling `python3`, `pip3`, etc</span>
</span></span></code></pre></div><p>Then, open VS Code and install its <a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python">Python extension</a></p><p>Then <code>git clone</code> Andrej&rsquo;s repositories locally:</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>git clone git@github.com:karpathy/micrograd.git
</span></span><span style=display:flex><span>git clone git@github.com:karpathy/makemore.git
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Open in VS Code</span>
</span></span><span style=display:flex><span>code micrograd
</span></span></code></pre></div><p>Open a Jupyter Notebook <code>.ipynb</code> file in <code>micrograd</code> and select the <code>pyenv</code> version you installed, plus install any VSCode recommended extensions for Jupyter Notebooks.</p><p>Create a new file called <code>youtube1.ipynb</code> or something similar so you can run same commands that Andrej does during his videos.</p><p>Now you&rsquo;re all set to dive into the videos.</p><h2 id=1-micrograd>1. Micrograd</h2><p>Watch the YouTube video: <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to neural networks and backpropagation: building micrograd</a>.</p><p>Code source: <a href=https://github.com/karpathy/micrograd>micrograd</a></p><p>And follow along locally in your own <code>.ipynb</code> file. I strongly recommend doing this yourself, typing out everything that Andrej does, and running it all locally. This &ldquo;practice&rdquo; helps me learn the content better and builds confidence that it can all be recreated locally.</p><p>Just for reference (create your own!), here are examples of local notebooks:</p><ul><li>My notebook from the <a href=https://github.com/BrianSigafoos/micrograd/blob/master/micrograd_youtube1.ipynb>1st part of the micrograd video</a>.</li><li>Two more complete <a href=https://github.com/Anri-Lombard/micrograd/tree/main/Lectures>notebooks for both parts of the micrograd video</a> by Github user @Anri-Lombard.</li></ul><p>Everything needed to understand a neural network is in <a href=https://github.com/karpathy/micrograd>micrograd</a>. Everything else is just efficiency.
There are only 150 lines of code in <code>micrograd/engine.py</code> and <code>micrograd/nn.py</code>.</p><p>&ldquo;Back propagation is recursive application of chain rule, backwards through the computation graph.&rdquo;</p><p>If can write local gradients and can do backward propagation of gradients, then it doesn&rsquo;t matter if it&rsquo;s a compound function or separate functions (equal to compound function), the result will be the same.</p><p><code>micrograd</code> is for scalar values (1.0, etc)
PyTorch is for a tensor, an n-dimensional array of scalars</p><p>Gradient descent is the iteration of:</p><ul><li>forward pass</li><li>backward pass</li><li>update</li></ul><p>The neural net improved its predictions with each iteration.</p><p>Most common neural net mistakes from <a href=https://twitter.com/karpathy/status/1013244313327681536>@karpathy&rsquo;s tweet</a>:</p><ol><li>you didn&rsquo;t try to overfit a single batch first.</li><li>you forgot to toggle train/eval mode for the net.</li><li>you forgot to .zero_grad() (in pytorch) before .backward().</li><li>you passed softmaxed outputs to a loss that expects raw logits.</li><li>you didn&rsquo;t use bias=False for your Linear/Conv2d layer when using BatchNorm, or conversely forget to include it for the output layer. This one won&rsquo;t make you silently fail, but they are spurious parameters</li><li>thinking view() and permute() are the same thing (& incorrectly using view)</li></ol><blockquote class=twitter-tweet><p lang=en dir=ltr>most common neural net mistakes: 1) you didn't try to overfit a single batch first. 2) you forgot to toggle train/eval mode for the net. 3) you forgot to .zero_grad() (in pytorch) before .backward(). 4) you passed softmaxed outputs to a loss that expects raw logits. ; others? :)</p>&mdash; Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/1013244313327681536?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><h2 id=2-makemore-part-1-bigram-character-level-language-model>2. Makemore part 1: bigram character-level language model</h2><p>Watch the YouTube video: <a href="https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to language modeling: building makemore</a>.</p><p>Code source: <a href=https://github.com/karpathy/makemore>makemore</a></p><p>Every line in <code>makemore/names.txt</code> is an example. Each example is a sequence of characters.
We&rsquo;re building a character level language model. It knows how to predict the next character in the sequence.</p><p>As noted in the video, it&rsquo;s important to learn more about <a href="https://pytorch.org/docs/stable/notes/broadcasting.html?highlight=broadcasting">Broadcasting semantics</a>.</p><p>This lecture let&rsquo;s us train a bigram language model.</p><h3 id=setup-code>Setup code</h3><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Setup - common to both approaches</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>torch</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># data set: 32k first names</span>
</span></span><span style=display:flex><span>words = <span style=color:#24909d>open</span>(<span style=color:#ed9d13>&#39;names.txt&#39;</span>, <span style=color:#ed9d13>&#39;r&#39;</span>).read().splitlines()
</span></span><span style=display:flex><span>chars = <span style=color:#24909d>sorted</span>(<span style=color:#24909d>list</span>(<span style=color:#24909d>set</span>(<span style=color:#ed9d13>&#39;&#39;</span>.join(words))))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># s to i lookup, setting `.` as 0 index in array and all others + 1</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># we&#39;ll use `.` to mark the start and end of all words</span>
</span></span><span style=display:flex><span>stoi = {s: i+<span style=color:#3677a9>1</span> <span style=color:#6ab825;font-weight:700>for</span> i, s <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(chars)}
</span></span><span style=display:flex><span>stoi[<span style=color:#ed9d13>&#39;.&#39;</span>] = <span style=color:#3677a9>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># i to s lookup</span>
</span></span><span style=display:flex><span>itos = {i: s <span style=color:#6ab825;font-weight:700>for</span> s, i <span style=color:#6ab825;font-weight:700>in</span> stoi.items()}
</span></span></code></pre></div><h3 id=non-neural-network-approach>Non-neural network approach</h3><p>We start training it by counting how frequently any pairing of letters occurs in ~32k names, and then normalizing so we get a nice probability distribution.</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Approach 1: non-neural network approach: count frequency of bigrams and store in `N`</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Create a 27x27 matrix with values all set to 0</span>
</span></span><span style=display:flex><span>N = torch.zeros((<span style=color:#3677a9>27</span>, <span style=color:#3677a9>27</span>), dtype=torch.int32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Get the counts</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> w <span style=color:#6ab825;font-weight:700>in</span> words:
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># use `.` to mark the start and end of all words</span>
</span></span><span style=display:flex><span>  chs = [<span style=color:#ed9d13>&#39;.&#39;</span>] + <span style=color:#24909d>list</span>(w) + [<span style=color:#ed9d13>&#39;.&#39;</span>]
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>for</span> ch1, ch2 <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>zip</span>(chs, chs[<span style=color:#3677a9>1</span>:]):
</span></span><span style=display:flex><span>    <span style=color:#999;font-style:italic># integer index of this character in stoi 0-27</span>
</span></span><span style=display:flex><span>    ix1 = stoi[ch1]
</span></span><span style=display:flex><span>    ix2 = stoi[ch2]
</span></span><span style=display:flex><span>    N[ix1, ix2] += <span style=color:#3677a9>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># prepare probabilities, parameters of our bigram language model</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Apply &#34;model smoothing&#34; using `N + 1` instead of `N`. This prevents 0&#39;s in probability matrix P, which could lead to `infinity` for loss measurement.</span>
</span></span><span style=display:flex><span>P = (N + <span style=color:#3677a9>1</span>).float()
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># 27, 27</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># 27, 1  # This is &#34;broadcastable&#34; and it stretches the 1 into all 27 rows</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># https://pytorch.org/docs/stable/notes/broadcasting.html?highlight=broadcasting</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Below uses `/=` to avoid creating new tensor, ie more efficient</span>
</span></span><span style=display:flex><span>P /= P.sum(<span style=color:#3677a9>1</span>, keepdim=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g = torch.Generator().manual_seed(<span style=color:#3677a9>2147483647</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Sample</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>5</span>):
</span></span><span style=display:flex><span>  out = []
</span></span><span style=display:flex><span>  ix = <span style=color:#3677a9>0</span>
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>while</span> <span style=color:#6ab825;font-weight:700>True</span>:
</span></span><span style=display:flex><span>    p = P[ix]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ix = torch.multinomial(p, num_samples=<span style=color:#3677a9>1</span>, replacement=<span style=color:#6ab825;font-weight:700>True</span>, generator=g).item()
</span></span><span style=display:flex><span>    out.append(itos[ix])
</span></span><span style=display:flex><span>    <span style=color:#999;font-style:italic># Break with `.` is found, marking the end of the word</span>
</span></span><span style=display:flex><span>    <span style=color:#6ab825;font-weight:700>if</span> ix == <span style=color:#3677a9>0</span>:
</span></span><span style=display:flex><span>      <span style=color:#6ab825;font-weight:700>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#24909d>print</span>(<span style=color:#ed9d13>&#39;&#39;</span>.join(out))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Output:</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   mor.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   axx.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   minaymoryles.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   kondlaisah.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   anchshizarie.</span>
</span></span></code></pre></div><h3 id=loss-function>Loss function</h3><p>Then we can evaluate the quality of this model.</p><p>Goal: summarize probabilities into a single number that measure the quality of this model.</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Goal: summarize probabilities into a single number that measure the quality of this model</span>
</span></span><span style=display:flex><span>log_likelihood = <span style=color:#3677a9>0.0</span>
</span></span><span style=display:flex><span>n = <span style=color:#3677a9>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> w <span style=color:#6ab825;font-weight:700>in</span> words:
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># for w in [&#34;andrejq&#34;]:</span>
</span></span><span style=display:flex><span>  chs = [<span style=color:#ed9d13>&#39;.&#39;</span>] + <span style=color:#24909d>list</span>(w) + [<span style=color:#ed9d13>&#39;.&#39;</span>]
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>for</span> ch1, ch2 <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>zip</span>(chs, chs[<span style=color:#3677a9>1</span>:]):
</span></span><span style=display:flex><span>    ix1 = stoi[ch1]
</span></span><span style=display:flex><span>    ix2 = stoi[ch2]
</span></span><span style=display:flex><span>    prob = P[ix1, ix2]
</span></span><span style=display:flex><span>    logprob = torch.log(prob)
</span></span><span style=display:flex><span>    <span style=color:#999;font-style:italic># This is because: log(a*b*c) = log(a) + log(b) + log(c)</span>
</span></span><span style=display:flex><span>    log_likelihood += logprob
</span></span><span style=display:flex><span>    n += <span style=color:#3677a9>1</span>
</span></span><span style=display:flex><span>    <span style=color:#24909d>print</span>(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>{</span>ch1<span style=color:#ed9d13>}{</span>ch2<span style=color:#ed9d13>}</span><span style=color:#ed9d13>: </span><span style=color:#ed9d13>{</span>prob<span style=color:#ed9d13>:</span><span style=color:#ed9d13>.4f</span><span style=color:#ed9d13>}</span><span style=color:#ed9d13> </span><span style=color:#ed9d13>{</span>logprob<span style=color:#ed9d13>:</span><span style=color:#ed9d13>.4f</span><span style=color:#ed9d13>}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>{</span>log_likelihood<span style=color:#ed9d13>=}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># negative log likelihood is a nice loss function.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># The lowest it can get is 0. The higher it is, the worse off the predictions</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># you are making are.</span>
</span></span><span style=display:flex><span>nll = -log_likelihood
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>{</span>nll<span style=color:#ed9d13>=}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Above was the sum negative log likelihood. Better is the average negative log likelihood.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># So divide that sum by `n` to get the average.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># So the loss function for the training set assigned by this model is 2.4. That&#39;s the &#34;quality&#34; of this model.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># The lower it is the better off we are. The higher it is the worse off we are.</span>
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>{</span>nll/n<span style=color:#ed9d13>}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Output:</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   log_likelihood=tensor(-559951.5625)</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   nll=tensor(559951.5625)</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   2.4543561935424805</span>
</span></span></code></pre></div><h3 id=neural-network-approach>Neural network approach</h3><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Approach 2: neural network approach trained on bigrams</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># for one hot encoding: `F.one_hot` below</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>torch.nn.functional</span> <span style=color:#6ab825;font-weight:700>as</span> <span style=color:#447fcf;text-decoration:underline>F</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Dataset: 228K bigrams from the 32K example names</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#</span>
</span></span><span style=display:flex><span>xs, ys = [], []
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> w <span style=color:#6ab825;font-weight:700>in</span> words:
</span></span><span style=display:flex><span>  chs = [<span style=color:#ed9d13>&#39;.&#39;</span>] + <span style=color:#24909d>list</span>(w) + [<span style=color:#ed9d13>&#39;.&#39;</span>]
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>for</span> ch1, ch2 <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>zip</span>(chs, chs[<span style=color:#3677a9>1</span>:]):
</span></span><span style=display:flex><span>    ix1 = stoi[ch1]
</span></span><span style=display:flex><span>    ix2 = stoi[ch2]
</span></span><span style=display:flex><span>    xs.append(ix1)
</span></span><span style=display:flex><span>    ys.append(ix2)
</span></span><span style=display:flex><span>xs = torch.tensor(xs)
</span></span><span style=display:flex><span>ys = torch.tensor(ys)
</span></span><span style=display:flex><span>num = xs.nelement()
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(<span style=color:#ed9d13>&#39;number of examples: &#39;</span>, num)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># initialize the &#39;network&#39;</span>
</span></span><span style=display:flex><span>g = torch.Generator().manual_seed(<span style=color:#3677a9>2147483647</span>)
</span></span><span style=display:flex><span>W = torch.randn((<span style=color:#3677a9>27</span>, <span style=color:#3677a9>27</span>), generator=g, requires_grad=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span></code></pre></div><p>Gradient descent</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Gradient descent</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> k <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>100</span>):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># forward pass</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># input to the network: one-hot encoding</span>
</span></span><span style=display:flex><span>  xenc = F.one_hot(xs, num_classes=<span style=color:#3677a9>27</span>).float()
</span></span><span style=display:flex><span>  logits = xenc @ W  <span style=color:#999;font-style:italic># predict log-counts</span>
</span></span><span style=display:flex><span>  counts = logits.exp()  <span style=color:#999;font-style:italic># counts, equivalent to N</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># probabilities for next character</span>
</span></span><span style=display:flex><span>  probs = counts / counts.sum(<span style=color:#3677a9>1</span>, keepdims=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># regularization loss: `0.01*(W**2).mean()` tries to make all W&#39;s 0</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># if `0.01` is higher it will be more uniform and not</span>
</span></span><span style=display:flex><span>  loss = -probs[torch.arange(num), ys].log().mean() + <span style=color:#3677a9>0.01</span> * (W**<span style=color:#3677a9>2</span>).mean()
</span></span><span style=display:flex><span>  <span style=color:#24909d>print</span>(loss.item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># backward pass</span>
</span></span><span style=display:flex><span>  W.grad = <span style=color:#6ab825;font-weight:700>None</span>  <span style=color:#999;font-style:italic># set to zero the gradient</span>
</span></span><span style=display:flex><span>  loss.backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># update</span>
</span></span><span style=display:flex><span>  W.data += -<span style=color:#3677a9>50</span> * W.grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Earlier we had 2.47 loss when we manually did the counts.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># So we&#39;d like this neural network approach to become as &#34;good&#34;, when measuring the loss.</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Sample from neural net model</span>
</span></span><span style=display:flex><span>g = torch.Generator().manual_seed(<span style=color:#3677a9>2147483647</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>5</span>):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  out = []
</span></span><span style=display:flex><span>  ix = <span style=color:#3677a9>0</span>
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>while</span> <span style=color:#6ab825;font-weight:700>True</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    xenc = F.one_hot(torch.tensor([ix]), num_classes=<span style=color:#3677a9>27</span>).float()
</span></span><span style=display:flex><span>    logits = xenc @ W  <span style=color:#999;font-style:italic># predict log-counts</span>
</span></span><span style=display:flex><span>    counts = logits.exp()  <span style=color:#999;font-style:italic># counts, equivalent to N</span>
</span></span><span style=display:flex><span>    p = counts / counts.sum(<span style=color:#3677a9>1</span>, keepdims=<span style=color:#6ab825;font-weight:700>True</span>) <span style=color:#999;font-style:italic># probabilities for next character</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ix = torch.multinomial(p, num_samples=<span style=color:#3677a9>1</span>, replacement=<span style=color:#6ab825;font-weight:700>True</span>, generator=g).item()
</span></span><span style=display:flex><span>    out.append(itos[ix])
</span></span><span style=display:flex><span>    <span style=color:#6ab825;font-weight:700>if</span> ix == <span style=color:#3677a9>0</span>:
</span></span><span style=display:flex><span>      <span style=color:#6ab825;font-weight:700>break</span>
</span></span><span style=display:flex><span>  <span style=color:#24909d>print</span>(<span style=color:#ed9d13>&#39;&#39;</span>.join(out))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># Output:  almost identical to approach 1 non-neural network with count frequencies</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   mor.</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   axx.</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   minaymoryles.</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   kondlaisah.</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   anchshizarie.</span>
</span></span></code></pre></div><h2 id=3-makemore-part-2-mlp>3. Makemore part 2: MLP</h2><p>Watch the YouTube video: <a href="https://www.youtube.com/watch?v=TCH_1BHY58I&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Building makemore Part 2: MLP</a>.</p><p>A multilayer perceptron (MLP) character-level language model.</p><p>Original paper: <a href=https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf>&ldquo;A Neural Probabilistic Language Model, Bengio et al. 2003&rdquo;</a></p><p>We can embed all of the integers in <code>X</code> as <code>C[X]</code> thanks to PyTorch multi dimensional indexing.</p><p>We want to randomly select some portion of the dataset, the minibatch. Then only forward, backward, and update on that minibatch.</p><p>What&rsquo;s the right learning rate? <a href="https://youtu.be/TCH_1BHY58I?t=2798">Video</a></p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>lre = torch.linspace(-<span style=color:#3677a9>3</span>, <span style=color:#3677a9>0</span>, <span style=color:#3677a9>1000</span>)
</span></span><span style=display:flex><span>lrs = <span style=color:#3677a9>10</span>**lre
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lri = []
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># Inside gradient descent</span>
</span></span><span style=display:flex><span>  lr = lrs[i]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># track stats</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#lri.append(lre[i])</span>
</span></span><span style=display:flex><span>  stepi.append(i)
</span></span><span style=display:flex><span>  lossi.append(loss.log10().item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># To get visualization</span>
</span></span><span style=display:flex><span>plt.plot(stepi, lossi)
</span></span></code></pre></div><p>Split up data into 3 splits:</p><ul><li><ol><li>training split - 80%</li></ol></li><li><ol start=2><li>dev/validation split - 10%</li></ol></li><li><ol start=3><li>test split - 10%</li></ol></li></ul><p>We use the training split to optimize the parameters of the model, like we&rsquo;re doing using gradient descent.</p><p>Use the dev/validation split to train the hyperparameters.</p><p>Use the test split to evaluate the performance of the model at the end. We should rarely use the test split to avoid overfitting to it and learning from it.</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Build dataset</span>
</span></span><span style=display:flex><span>block_size = <span style=color:#3677a9>3</span> <span style=color:#999;font-style:italic># context length: how many characters do we take to predict the next one?</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>build_dataset</span>(words):
</span></span><span style=display:flex><span>  X, Y = [], []
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>for</span> w <span style=color:#6ab825;font-weight:700>in</span> words:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#999;font-style:italic># print(w)</span>
</span></span><span style=display:flex><span>    context = [<span style=color:#3677a9>0</span>] * block_size
</span></span><span style=display:flex><span>    <span style=color:#6ab825;font-weight:700>for</span> ch <span style=color:#6ab825;font-weight:700>in</span> w + <span style=color:#ed9d13>&#39;.&#39;</span>:
</span></span><span style=display:flex><span>      ix = stoi[ch]
</span></span><span style=display:flex><span>      X.append(context)
</span></span><span style=display:flex><span>      Y.append(ix)
</span></span><span style=display:flex><span>      <span style=color:#999;font-style:italic># print(&#39;&#39;.join(itos[i] for i in context), &#39;---&gt;&#39;, itos[ix])</span>
</span></span><span style=display:flex><span>      context = context[<span style=color:#3677a9>1</span>:] + [ix] <span style=color:#999;font-style:italic># crop and append</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  X = torch.tensor(X)
</span></span><span style=display:flex><span>  Y = torch.tensor(Y)
</span></span><span style=display:flex><span>  <span style=color:#24909d>print</span>(X.shape, Y.shape)
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>return</span> X, Y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>random</span>
</span></span><span style=display:flex><span>random.seed(<span style=color:#3677a9>42</span>)
</span></span><span style=display:flex><span>random.shuffle(words)
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Get n1 and n2 to help us split up the words into 3 splits.</span>
</span></span><span style=display:flex><span>n1 = <span style=color:#24909d>int</span>(<span style=color:#3677a9>0.8</span>*<span style=color:#24909d>len</span>(words))
</span></span><span style=display:flex><span>n2 = <span style=color:#24909d>int</span>(<span style=color:#3677a9>0.9</span>*<span style=color:#24909d>len</span>(words))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Split up data into 3 splits:</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># - 1. training split - 80%</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># - 2. dev/validation split - 10%</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># - 3. test split - 10%</span>
</span></span><span style=display:flex><span>Xtr, Ytr = build_dataset(words[:n1])     <span style=color:#999;font-style:italic># 80%</span>
</span></span><span style=display:flex><span>Xdev, Ydev = build_dataset(words[n1:n2]) <span style=color:#999;font-style:italic># 10%</span>
</span></span><span style=display:flex><span>Xte, Yte = build_dataset(words[n2:])     <span style=color:#999;font-style:italic># 10%</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># MLP revisited (from start of &#34;building makemore part 3&#34;)</span>
</span></span><span style=display:flex><span>n_embd = <span style=color:#3677a9>10</span> <span style=color:#999;font-style:italic># the dimensionality of the character embedding vectors</span>
</span></span><span style=display:flex><span>n_hidden = <span style=color:#3677a9>200</span> <span style=color:#999;font-style:italic># the number of neurons in the hidden layer of the MLP</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g = torch.Generator().manual_seed(<span style=color:#3677a9>2147483647</span>) <span style=color:#999;font-style:italic># for reproducibility</span>
</span></span><span style=display:flex><span>C  = torch.randn((vocab_size, n_embd),            generator=g)
</span></span><span style=display:flex><span>W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)
</span></span><span style=display:flex><span>b1 = torch.randn(n_hidden,                        generator=g)
</span></span><span style=display:flex><span>W2 = torch.randn((n_hidden, vocab_size),          generator=g)
</span></span><span style=display:flex><span>b2 = torch.randn(vocab_size,                      generator=g)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># BatchNorm parameters</span>
</span></span><span style=display:flex><span>bngain = torch.ones((<span style=color:#3677a9>1</span>, n_hidden))
</span></span><span style=display:flex><span>bnbias = torch.zeros((<span style=color:#3677a9>1</span>, n_hidden))
</span></span><span style=display:flex><span>bnmean_running = torch.zeros((<span style=color:#3677a9>1</span>, n_hidden))
</span></span><span style=display:flex><span>bnstd_running = torch.ones((<span style=color:#3677a9>1</span>, n_hidden))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>parameters = [C, W1, b1, W2, b2]
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(<span style=color:#24909d>sum</span>(p.nelement() <span style=color:#6ab825;font-weight:700>for</span> p <span style=color:#6ab825;font-weight:700>in</span> parameters)) <span style=color:#999;font-style:italic># number of parameters in total</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> p <span style=color:#6ab825;font-weight:700>in</span> parameters:
</span></span><span style=display:flex><span>  p.requires_grad = <span style=color:#6ab825;font-weight:700>True</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>max_steps = <span style=color:#3677a9>200000</span>
</span></span><span style=display:flex><span>batch_size = <span style=color:#3677a9>32</span>
</span></span><span style=display:flex><span>lossi = []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(max_steps):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># minibatch construct</span>
</span></span><span style=display:flex><span>  ix = torch.randint(<span style=color:#3677a9>0</span>, Xtr.shape[<span style=color:#3677a9>0</span>], (batch_size,), generator=g)
</span></span><span style=display:flex><span>  Xb, Yb = Xtr[ix], Ytr[ix] <span style=color:#999;font-style:italic># batch X,Y</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># forward pass</span>
</span></span><span style=display:flex><span>  emb = C[Xb] <span style=color:#999;font-style:italic># embed the characters into vectors</span>
</span></span><span style=display:flex><span>  embcat = emb.view(emb.shape[<span style=color:#3677a9>0</span>], -<span style=color:#3677a9>1</span>) <span style=color:#999;font-style:italic># concatenate the vectors</span>
</span></span><span style=display:flex><span>  hpreact = embcat @ W1 + b1 <span style=color:#999;font-style:italic># hidden layer pre-activation</span>
</span></span><span style=display:flex><span>  h = torch.tanh(hpreact) <span style=color:#999;font-style:italic># hidden layer</span>
</span></span><span style=display:flex><span>  logits = h @ W2 + b2 <span style=color:#999;font-style:italic># output layer</span>
</span></span><span style=display:flex><span>  loss = F.cross_entropy(logits, Yb) <span style=color:#999;font-style:italic># loss function</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># backward pass</span>
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>for</span> p <span style=color:#6ab825;font-weight:700>in</span> parameters:
</span></span><span style=display:flex><span>    p.grad = <span style=color:#6ab825;font-weight:700>None</span>
</span></span><span style=display:flex><span>  loss.backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># update</span>
</span></span><span style=display:flex><span>  lr = <span style=color:#3677a9>0.1</span> <span style=color:#6ab825;font-weight:700>if</span> i &lt; <span style=color:#3677a9>100000</span> <span style=color:#6ab825;font-weight:700>else</span> <span style=color:#3677a9>0.01</span> <span style=color:#999;font-style:italic># step learning rate decay</span>
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>for</span> p <span style=color:#6ab825;font-weight:700>in</span> parameters:
</span></span><span style=display:flex><span>    p.data += -lr * p.grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># track stats</span>
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>if</span> i % <span style=color:#3677a9>10000</span> == <span style=color:#3677a9>0</span>: <span style=color:#999;font-style:italic># print every once in a while</span>
</span></span><span style=display:flex><span>    <span style=color:#24909d>print</span>(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>{</span>i<span style=color:#ed9d13>:</span><span style=color:#ed9d13>7d</span><span style=color:#ed9d13>}</span><span style=color:#ed9d13>/</span><span style=color:#ed9d13>{</span>max_steps<span style=color:#ed9d13>:</span><span style=color:#ed9d13>7d</span><span style=color:#ed9d13>}</span><span style=color:#ed9d13>: </span><span style=color:#ed9d13>{</span>loss.item()<span style=color:#ed9d13>:</span><span style=color:#ed9d13>.4f</span><span style=color:#ed9d13>}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  lossi.append(loss.log10().item())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Evaluate</span>
</span></span><span style=display:flex><span><span style=color:orange>@torch.no_grad</span>() <span style=color:#999;font-style:italic># this decorator disables gradient tracking</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>split_loss</span>(split):
</span></span><span style=display:flex><span>  x,y = {
</span></span><span style=display:flex><span>    <span style=color:#ed9d13>&#39;train&#39;</span>: (Xtr, Ytr),
</span></span><span style=display:flex><span>    <span style=color:#ed9d13>&#39;val&#39;</span>: (Xdev, Ydev),
</span></span><span style=display:flex><span>    <span style=color:#ed9d13>&#39;test&#39;</span>: (Xte, Yte),
</span></span><span style=display:flex><span>  }[split]
</span></span><span style=display:flex><span>  emb = C[x] <span style=color:#999;font-style:italic># (N, block_size, n_embd)</span>
</span></span><span style=display:flex><span>  embcat = emb.view(emb.shape[<span style=color:#3677a9>0</span>], -<span style=color:#3677a9>1</span>) <span style=color:#999;font-style:italic># concat into (N, block_size * n_embd)</span>
</span></span><span style=display:flex><span>  h = torch.tanh(embcat @ W1 + b1) <span style=color:#999;font-style:italic># (N, n_hidden)</span>
</span></span><span style=display:flex><span>  logits = h @ W2 + b2 <span style=color:#999;font-style:italic># (N, vocab_size)</span>
</span></span><span style=display:flex><span>  loss = F.cross_entropy(logits, y)
</span></span><span style=display:flex><span>  <span style=color:#24909d>print</span>(split, loss.item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>split_loss(<span style=color:#ed9d13>&#39;train&#39;</span>)
</span></span><span style=display:flex><span>split_loss(<span style=color:#ed9d13>&#39;val&#39;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Sample from the model</span>
</span></span><span style=display:flex><span>g = torch.Generator().manual_seed(<span style=color:#3677a9>2147483647</span> + <span style=color:#3677a9>10</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> _ <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>20</span>):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    out = []
</span></span><span style=display:flex><span>    context = [<span style=color:#3677a9>0</span>] * block_size <span style=color:#999;font-style:italic># initialize with all ...</span>
</span></span><span style=display:flex><span>    <span style=color:#6ab825;font-weight:700>while</span> <span style=color:#6ab825;font-weight:700>True</span>:
</span></span><span style=display:flex><span>      <span style=color:#999;font-style:italic># forward pass the neural net</span>
</span></span><span style=display:flex><span>      emb = C[torch.tensor([context])] <span style=color:#999;font-style:italic># (1,block_size,n_embd)</span>
</span></span><span style=display:flex><span>      h = torch.tanh(emb.view(<span style=color:#3677a9>1</span>, -<span style=color:#3677a9>1</span>) @ W1 + b1)
</span></span><span style=display:flex><span>      logits = h @ W2 + b2
</span></span><span style=display:flex><span>      probs = F.softmax(logits, dim=<span style=color:#3677a9>1</span>)
</span></span><span style=display:flex><span>      <span style=color:#999;font-style:italic># sample from the distribution</span>
</span></span><span style=display:flex><span>      ix = torch.multinomial(probs, num_samples=<span style=color:#3677a9>1</span>, generator=g).item()
</span></span><span style=display:flex><span>      <span style=color:#999;font-style:italic># shift the context window and track the samples</span>
</span></span><span style=display:flex><span>      context = context[<span style=color:#3677a9>1</span>:] + [ix]
</span></span><span style=display:flex><span>      out.append(ix)
</span></span><span style=display:flex><span>      <span style=color:#999;font-style:italic># if we sample the special &#39;.&#39; token, break</span>
</span></span><span style=display:flex><span>      <span style=color:#6ab825;font-weight:700>if</span> ix == <span style=color:#3677a9>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#6ab825;font-weight:700>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#24909d>print</span>(<span style=color:#ed9d13>&#39;&#39;</span>.join(itos[i] <span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> out)) <span style=color:#999;font-style:italic># decode and print the generated word</span>
</span></span></code></pre></div><p>Underfitting - the loss function for the training split is similar to the dev/validation split. Likely too few parameters and the model isn&rsquo;t powerful enough to</p><p>Iterating the model:</p><ul><li>Runs lots of experiments changing hyperparameters (learning rate, minibatch size, etc) on dev set and slowly scrutinize which ones give the best dev performance</li><li>Then run 1x on the test set and that&rsquo;s the loss rate number that matters when publishing, sharing results of the model.</li></ul><h2 id=4-makemore-part-3-activations--gradients-batchnorm>4. Makemore part 3: Activations & Gradients, BatchNorm</h2><p>Watch the Youtube video: <a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Building makemore Part 3: Activations & Gradients, BatchNorm</a></p><p>Fixing problems from the previous MLP model.</p><h3 id=problem-1-initial-loss-is-off-aka-softmax-confidently-wrong>Problem 1: initial loss is off aka softmax confidently wrong</h3><p>Symptom: there is a hockey stick of loss from very high to a tighter range.</p><p>We&rsquo;d expect the loss for the newly initialized model with 27 characters to be <code>-torch.tensor(1/27.0).log() == 3.2958</code>. Instead we&rsquo;re seeing ~27. Something is wrong with the initialization.</p><p>When the network is initialized we want the logits to be roughly 0 or 1 when it is initialized. To debug, insert <code>break</code> after the first pass of gradient descent and inspect <code>logits[0]</code>. We see there extreme values.</p><p>Solution: multiple the weights and bias by 0.01 and 0 to get logits closer to 0 to start.</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W2 = torch.randn((n_hidden, vocab_size), generator=g) * <span style=color:#3677a9>0.01</span> <span style=color:#999;font-style:italic># Fix to get initial logits closer to 0</span>
</span></span><span style=display:flex><span>b2 = torch.randn(vocab_size,             generator=g) * <span style=color:#3677a9>0</span>    <span style=color:#999;font-style:italic># Fix to get initial logits closer to 0</span>
</span></span></code></pre></div><h3 id=problem-2-tanh-layer-too-saturated-at-init>Problem 2: tanh layer too saturated at init</h3><p>Sympton: there are too many -1&rsquo;s and 1&rsquo;s in the <code>.tanh</code>. It&rsquo;s too saturated</p><p>Help diagnose by using a histogram of the values:</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># h = torch.tanh(hpreact) # hidden layer</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># flattened with h.view(-1); split into 50 buckets/bins</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Figure 1.</span>
</span></span><span style=display:flex><span>plt.hist(h.view(-<span style=color:#3677a9>1</span>).tolist(), <span style=color:#3677a9>50</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Figure 2</span>
</span></span><span style=display:flex><span>plt.figure(figsize=(<span style=color:#3677a9>20</span>,<span style=color:#3677a9>10</span>))
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># will appear white if boolean (h.abs() &gt; 0.99)) is true, ie when it&#39;s in the long tail</span>
</span></span><span style=display:flex><span>plt.imshow(h.abs() &gt; <span style=color:#3677a9>0.99</span>, cmap=<span style=color:#ed9d13>&#39;gray&#39;</span>, interpolation=<span style=color:#ed9d13>&#39;nearest&#39;</span>)
</span></span></code></pre></div><p>If an entire column is white, then that&rsquo;s a &ldquo;dead neuron&rdquo;. All the examples land in the tail, then it will never learn as a neuron.</p><p>The neuron will &ldquo;stop&rdquo; backpropogation.</p><p>Solution: similar to above, tighten the range of values this time in W1 and b1 by multiplying by 0.1 or 0.01 or some other decimal value.</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * <span style=color:#3677a9>0.2</span>
</span></span><span style=display:flex><span>b1 = torch.randn(n_hidden,                        generator=g) * <span style=color:#3677a9>0.01</span>
</span></span></code></pre></div><p>Instead of just guessing at these values <code>0.2</code> we can use the <a href=https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_>kaiming initialization</a></p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (<span style=color:#3677a9>5</span>/<span style=color:#3677a9>3</span>)/((n_embd * block_size)**<span style=color:#3677a9>0.5</span>) <span style=color:#999;font-style:italic># kaiming init</span>
</span></span><span style=display:flex><span>b1 = torch.randn(n_hidden,                        generator=g) * <span style=color:#3677a9>0.01</span>
</span></span></code></pre></div><h3 id=problem-3-batch-normalization>Problem 3: batch normalization</h3><p>We can make the hidden states Gaussian by standardizing them to make them exactly Gaussian, at initialization only.</p><p>It happens to have a regularizing effect and stabilizes training. People are trying to remove batch normalization, but it works quite well &ndash; it is quite effective at controlling activations and their distributions.</p><p>The batch normalization layer has its own bias. And there&rsquo;s no need to have a bias (eg. <code>+ b1</code>) in the layer before it.</p><p>Summary:</p><ul><li>We use batch normalization to control the statistics of activations in the neural net</li><li>It is common to sprinkle batch normalization layer across the neural net</li><li>Usually we will place it after layers that have multiplications (a linear layer or convolutional layer)</li><li>Bactch normalization internally has parameters for the gain and the bias and these are trained using backpropagation</li><li>Batch normalization has two buffers: the running mean and the running standing deviation, and these buffers are not trained using backpropagation, trained using the &ldquo;janky&rdquo; running updates</li><li>In BatchNorm layer:<ul><li>Calculating the mean and standard deviation of the activations over that batch</li><li>Then centering that batch to be unit gaussian</li><li>Then offsetting and scaling it be the learned bias and gain</li><li>And additionally keeping a running mean and standard deviation so we don&rsquo;t have to re-estimate it all the time, and this let&rsquo;s us forward individual examples (not a batch) at test time.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(max_steps):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># minibatch construct</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># forward pass</span>
</span></span><span style=display:flex><span>  emb = C[Xb] <span style=color:#999;font-style:italic># embed the characters into vectors</span>
</span></span><span style=display:flex><span>  embcat = emb.view(emb.shape[<span style=color:#3677a9>0</span>], -<span style=color:#3677a9>1</span>) <span style=color:#999;font-style:italic># concatenate the vectors</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># Linear layer</span>
</span></span><span style=display:flex><span>  hpreact = embcat @ W1 <span style=color:#999;font-style:italic>#+ b1 # hidden layer pre-activation</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># BatchNorm layer</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># -------------------------------------------------------------</span>
</span></span><span style=display:flex><span>  bnmeani = hpreact.mean(<span style=color:#3677a9>0</span>, keepdim=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span><span style=display:flex><span>  bnstdi = hpreact.std(<span style=color:#3677a9>0</span>, keepdim=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span><span style=display:flex><span>  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>with</span> torch.no_grad():
</span></span><span style=display:flex><span>    bnmean_running = <span style=color:#3677a9>0.999</span> * bnmean_running + <span style=color:#3677a9>0.001</span> * bnmeani
</span></span><span style=display:flex><span>    bnstd_running = <span style=color:#3677a9>0.999</span> * bnstd_running + <span style=color:#3677a9>0.001</span> * bnstdi
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># -------------------------------------------------------------</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># Non-linearity</span>
</span></span><span style=display:flex><span>  h = torch.tanh(hpreact) <span style=color:#999;font-style:italic># hidden layer</span>
</span></span><span style=display:flex><span>  logits = h @ W2 + b2 <span style=color:#999;font-style:italic># output layer</span>
</span></span><span style=display:flex><span>  loss = F.cross_entropy(logits, Yb) <span style=color:#999;font-style:italic># loss function</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># backward pass ...</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># update ...</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># track stats ...</span>
</span></span></code></pre></div><h3 id=pytorch-ify-the-code-so-far>PyTorch-ify the code so far</h3><p>In our code, we&rsquo;ll mimic the PyTorch code for:</p><ul><li><a href=https://pytorch.org/docs/stable/generated/torch.nn.Linear.html>torch.nn.linear</a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html>torch.nn.BatchNorm1d</a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html>torch.nn.Tanh</a></li></ul><p>See <code>class Linear:</code>, <code>class BatchNorm1d:</code>, etc here: <a href=https://github.com/BrianSigafoos/makemore/blob/4d51f23df80f6757809884a9d2e2888c835507f4/makemore_part3_bn.ipynb#L4>https://github.com/BrianSigafoos/makemore/blob/4d51f23df80f6757809884a9d2e2888c835507f4/makemore_part3_bn.ipynb#L4</a></p><h3 id=diagnostic-tools>Diagnostic tools</h3><p>To understand if neural network is in a good state.</p><p>Activation distribution from forward pass</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt.figure(figsize=(<span style=color:#3677a9>20</span>, <span style=color:#3677a9>4</span>)) <span style=color:#999;font-style:italic># width and height of the plot</span>
</span></span><span style=display:flex><span>legends = []
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i, layer <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(layers[:-<span style=color:#3677a9>1</span>]): <span style=color:#999;font-style:italic># note: exclude the output layer</span>
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>if</span> <span style=color:#24909d>isinstance</span>(layer, Tanh):
</span></span><span style=display:flex><span>    t = layer.out
</span></span><span style=display:flex><span>    <span style=color:#24909d>print</span>(<span style=color:#ed9d13>&#39;layer </span><span style=color:#ed9d13>%d</span><span style=color:#ed9d13> (</span><span style=color:#ed9d13>%10s</span><span style=color:#ed9d13>): mean </span><span style=color:#ed9d13>%+.2f</span><span style=color:#ed9d13>, std </span><span style=color:#ed9d13>%.2f</span><span style=color:#ed9d13>, saturated: </span><span style=color:#ed9d13>%.2f%%</span><span style=color:#ed9d13>&#39;</span> % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; <span style=color:#3677a9>0.97</span>).float().mean()*<span style=color:#3677a9>100</span>))
</span></span><span style=display:flex><span>    hy, hx = torch.histogram(t, density=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span><span style=display:flex><span>    plt.plot(hx[:-<span style=color:#3677a9>1</span>].detach(), hy.detach())
</span></span><span style=display:flex><span>    legends.append(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;layer </span><span style=color:#ed9d13>{</span>i<span style=color:#ed9d13>}</span><span style=color:#ed9d13> (</span><span style=color:#ed9d13>{</span>layer.__class__.__name__<span style=color:#ed9d13>}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span>plt.legend(legends);
</span></span><span style=display:flex><span>plt.title(<span style=color:#ed9d13>&#39;activation distribution&#39;</span>)
</span></span></code></pre></div><p>Gradient distribution from backward pass</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt.figure(figsize=(<span style=color:#3677a9>20</span>, <span style=color:#3677a9>4</span>)) <span style=color:#999;font-style:italic># width and height of the plot</span>
</span></span><span style=display:flex><span>legends = []
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i, layer <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(layers[:-<span style=color:#3677a9>1</span>]): <span style=color:#999;font-style:italic># note: exclude the output layer</span>
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>if</span> <span style=color:#24909d>isinstance</span>(layer, Tanh):
</span></span><span style=display:flex><span>    t = layer.out.grad
</span></span><span style=display:flex><span>    <span style=color:#24909d>print</span>(<span style=color:#ed9d13>&#39;layer </span><span style=color:#ed9d13>%d</span><span style=color:#ed9d13> (</span><span style=color:#ed9d13>%10s</span><span style=color:#ed9d13>): mean </span><span style=color:#ed9d13>%+f</span><span style=color:#ed9d13>, std </span><span style=color:#ed9d13>%e</span><span style=color:#ed9d13>&#39;</span> % (i, layer.__class__.__name__, t.mean(), t.std()))
</span></span><span style=display:flex><span>    hy, hx = torch.histogram(t, density=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span><span style=display:flex><span>    plt.plot(hx[:-<span style=color:#3677a9>1</span>].detach(), hy.detach())
</span></span><span style=display:flex><span>    legends.append(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;layer </span><span style=color:#ed9d13>{</span>i<span style=color:#ed9d13>}</span><span style=color:#ed9d13> (</span><span style=color:#ed9d13>{</span>layer.__class__.__name__<span style=color:#ed9d13>}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span>plt.legend(legends);
</span></span><span style=display:flex><span>plt.title(<span style=color:#ed9d13>&#39;gradient distribution&#39;</span>)
</span></span></code></pre></div><p>Weights gradient distribution from stochastic gradient descent</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># visualize histograms</span>
</span></span><span style=display:flex><span>plt.figure(figsize=(<span style=color:#3677a9>20</span>, <span style=color:#3677a9>4</span>)) <span style=color:#999;font-style:italic># width and height of the plot</span>
</span></span><span style=display:flex><span>legends = []
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i,p <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(parameters):
</span></span><span style=display:flex><span>  t = p.grad
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>if</span> p.ndim == <span style=color:#3677a9>2</span>:
</span></span><span style=display:flex><span>    <span style=color:#24909d>print</span>(<span style=color:#ed9d13>&#39;weight </span><span style=color:#ed9d13>%10s</span><span style=color:#ed9d13> | mean </span><span style=color:#ed9d13>%+f</span><span style=color:#ed9d13> | std </span><span style=color:#ed9d13>%e</span><span style=color:#ed9d13> | grad:data ratio </span><span style=color:#ed9d13>%e</span><span style=color:#ed9d13>&#39;</span> % (<span style=color:#24909d>tuple</span>(p.shape), t.mean(), t.std(), t.std() / p.std()))
</span></span><span style=display:flex><span>    hy, hx = torch.histogram(t, density=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span><span style=display:flex><span>    plt.plot(hx[:-<span style=color:#3677a9>1</span>].detach(), hy.detach())
</span></span><span style=display:flex><span>    legends.append(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>{</span>i<span style=color:#ed9d13>}</span><span style=color:#ed9d13> </span><span style=color:#ed9d13>{</span><span style=color:#24909d>tuple</span>(p.shape)<span style=color:#ed9d13>}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span>plt.legend(legends)
</span></span><span style=display:flex><span>plt.title(<span style=color:#ed9d13>&#39;weights gradient distribution&#39;</span>);
</span></span></code></pre></div><p>Updates distribution from stochastic gradient descent</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt.figure(figsize=(<span style=color:#3677a9>20</span>, <span style=color:#3677a9>4</span>))
</span></span><span style=display:flex><span>legends = []
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i,p <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(parameters):
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>if</span> p.ndim == <span style=color:#3677a9>2</span>:
</span></span><span style=display:flex><span>    plt.plot([ud[j][i] <span style=color:#6ab825;font-weight:700>for</span> j <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#24909d>len</span>(ud))])
</span></span><span style=display:flex><span>    legends.append(<span style=color:#ed9d13>&#39;param </span><span style=color:#ed9d13>%d</span><span style=color:#ed9d13>&#39;</span> % i)
</span></span><span style=display:flex><span>plt.plot([<span style=color:#3677a9>0</span>, <span style=color:#24909d>len</span>(ud)], [-<span style=color:#3677a9>3</span>, -<span style=color:#3677a9>3</span>], <span style=color:#ed9d13>&#39;k&#39;</span>) <span style=color:#999;font-style:italic># these ratios should be ~1e-3, indicate on plot</span>
</span></span><span style=display:flex><span>plt.legend(legends);
</span></span></code></pre></div><p>Updates</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Inside gradient descent</span>
</span></span><span style=display:flex><span>ud = []
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(max_steps):
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># track stats ...</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># if i % 10000 == 0: # print every once in a while</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   print(f&#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}&#39;)</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># lossi.append(loss.log10().item())</span>
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>with</span> torch.no_grad():
</span></span><span style=display:flex><span>    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() <span style=color:#6ab825;font-weight:700>for</span> p <span style=color:#6ab825;font-weight:700>in</span> parameters])
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># ---</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Histogram</span>
</span></span><span style=display:flex><span>plt.figure(figsize=(<span style=color:#3677a9>20</span>, <span style=color:#3677a9>4</span>))
</span></span><span style=display:flex><span>legends = []
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i,p <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(parameters):
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>if</span> p.ndim == <span style=color:#3677a9>2</span>:
</span></span><span style=display:flex><span>    plt.plot([ud[j][i] <span style=color:#6ab825;font-weight:700>for</span> j <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#24909d>len</span>(ud))])
</span></span><span style=display:flex><span>    legends.append(<span style=color:#ed9d13>&#39;param </span><span style=color:#ed9d13>%d</span><span style=color:#ed9d13>&#39;</span> % i)
</span></span><span style=display:flex><span>plt.plot([<span style=color:#3677a9>0</span>, <span style=color:#24909d>len</span>(ud)], [-<span style=color:#3677a9>3</span>, -<span style=color:#3677a9>3</span>], <span style=color:#ed9d13>&#39;k&#39;</span>) <span style=color:#999;font-style:italic># these ratios should be ~1e-3, indicate on plot</span>
</span></span><span style=display:flex><span>plt.legend(legends);
</span></span></code></pre></div><p>When creating a deep neural network the &ldquo;motif&rdquo; to stack up for the &ldquo;forward&rdquo; pass is:</p><ul><li>convolution / linear layer (weight layer)</li><li>normalization layer (batch normalization, group normalization, layer normalization)</li><li>non-linearity (relu, tanh, etc)</li></ul><h2 id=5-building-makemore-part-4-becoming-a-backprop-ninja>5. Building makemore Part 4: Becoming a Backprop Ninja</h2><p>Watch the video on YouTube <a href="https://www.youtube.com/watch?v=q8SA3rM6ckI&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Building makemore Part 4: Becoming a Backprop Ninja</a> and follow along in the <a href="https://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing">Google Collab</a></p><p>It&rsquo;s worth doing as much as possible yourself to help understand what goes on under the hood when calling <code>.backward</code> in PyTorch.</p><h2 id=6-building-makemore-part-5-building-a-wavenet>6. Building makemore Part 5: Building a WaveNet</h2><p>Watch the video on YouTube: <a href="https://www.youtube.com/watch?v=t3YJ5hKiMQ0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Building makemore Part 5: Building a WaveNet</a></p><p>Implements a convolutional neural network architecture similar to <a href=https://arxiv.org/abs/1609.03499>WaveNet (2016 paper, DeepMind)</a>.</p><p>We want to make the network deeper. And at each level we want to fuse two consequetive elements.</p><p>Debug code to view the name and shape of each layer:</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Layer inspector - code snippet to help debug</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> layer <span style=color:#6ab825;font-weight:700>in</span> model.layers:
</span></span><span style=display:flex><span>  <span style=color:#24909d>print</span>(layer.__class__.__name__, <span style=color:#ed9d13>&#39;:&#39;</span>, <span style=color:#24909d>tuple</span>(layer.out.shape))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># For example:</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   Embedding : (32, 8, 10)</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   FlattenConsecutive : (32, 80)</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   Linear : (32, 200)</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   BatchNorm1d : (32, 200)</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   Tanh : (32, 200)</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   Linear : (32, 27)</span>
</span></span></code></pre></div><p>We can switch to using <code>torch.nn</code> moving forward now that in this video we got a better understanding of how the stack layers and mimic the PyTorch API.</p><p>What&rsquo;s the development process like for building a deep neural network</p><ol><li>Spending a lot of time in the PyTorch documentation. Looking at the layers, the shapes of the inputs, what the layer does, etc.</li><li>Trying to make the shapes work, and &ldquo;gymnastics&rdquo; around multi-dimensional arrays.</li><li>Prototype shapes and layers in Jupyter notebooks to make sure it works out, then copy and paste code into actual code repository</li></ol><p>Future:</p><ul><li>We need to set up an experimental/evaluation harness to kick off lots of experiments, hyperparameter searches.</li></ul><h2 id=7-gpt-from-scratch>7. GPT from scratch</h2><p>Watch the video on YouTube: <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let&rsquo;s build GPT: from scratch, in code, spelled out</a></p><p>Typically sub-word tokenizers have large dictionaries (~50k tokens), like:</p><ul><li><a href=https://github.com/google/sentencepiece>google/sentencepiece</a></li><li><a href=https://github.com/openai/tiktoken>openai/tiktoken</a></li></ul><p>Instead, we&rsquo;ll have 65 characters simply encoded at the character level to keep this example simple.</p><p>In this example, the later nodes get information from the earlier nodes but never from the future nodes.</p><p>&ldquo;Attention can be applied to any arbitrary directed graph. Attention is just a communication mechanism between the nodes. &mldr; An attention is just a set of vectors out there in space. They communicate. And if you want them to have a notion of space, you need to specifically add it.&rdquo; &ldquo;This is why we need to positionally encode tokens.&rdquo; (see notes in <a href="https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=M5CvobiQ0pLr">collab notebook</a>)</p><p>&ldquo;Attention supports arbitrary connectivity between nodes.&rdquo;</p><ul><li>An &ldquo;encoder&rdquo; attention block would allow nodes to communicate with each other.</li><li>A &ldquo;decoder&rdquo; attention block has triangular masking (as in this example) and is used in an autoregressive setting typically, like language modeling.</li></ul><h3 id=dropout>Dropout</h3><p>Using &ldquo;dropout&rdquo; is like training an ensemble of subnetworks because during training connections are randomly dropped. More from the paper <a href=https://dl.acm.org/doi/abs/10.5555/2627435.2670313>Dropout: a simple way to prevent neural networks from overfitting</a>:</p><blockquote><p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different &ldquo;thinned&rdquo; networks.</p></blockquote><h2 id=pytorch-tips>PyTorch Tips</h2><p>PyTorch <a href=https://pytorch.org/docs/stable/torch.html>docs</a></p><h3 id=tensor-views---docshttpspytorchorgdocsstabletensor_viewhtml>Tensor Views - <a href=https://pytorch.org/docs/stable/tensor_view.html>docs</a></h3><p>PyTorch allows a tensor to be a View of an existing tensor. View tensor shares the same underlying data with its base tensor. Supporting View avoids explicit data copy, thus allows us to do fast and memory efficient reshaping, slicing and element-wise operations.</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>t = torch.arange(<span style=color:#3677a9>8</span>)
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(t)
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(t.view(<span style=color:#3677a9>2</span>, <span style=color:#3677a9>4</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Output:</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   tensor([0, 1, 2, 3, 4, 5, 6, 7])</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   tensor([[0, 1, 2, 3],</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#           [4, 5, 6, 7]])</span>
</span></span></code></pre></div><h3 id=fcross_entropy---docshttpspytorchorgdocsstablegeneratedtorchnnfunctionalcross_entropyhtmlhighlightcross_entropytorchnnfunctionalcross_entropy><code>F.cross_entropy</code> - <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html?highlight=cross_entropy#torch.nn.functional.cross_entropy">docs</a></h3><p>Instead of hand rolling the commented out code below, calling <code>F.cross_entropy</code> makes the forward and backward passes much more efficient.</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># counts = logits.exp()</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># prob = counts / counts.sum(1, keepdimes=True)</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># loss = -prob[torch.arange(32), Y].log().mean()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Replaced efficiently with:</span>
</span></span><span style=display:flex><span>F.cross_entropy(logits, Y)
</span></span></code></pre></div><h2 id=references>References</h2><p>Github repos:</p><ul><li><a href=https://github.com/karpathy/micrograd>micrograd</a> - A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API</li><li><a href=https://github.com/karpathy/makemore>makemore</a> - An autoregressive character-level language model for making more things</li></ul><p>Articles:</p><ul><li><a href=https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b>Karpathy: Yes you should understand backprop</a></li></ul><p>More:</p><ul><li><a href="https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC">CS231n Winter 2016 - Karpathy lectures</a></li></ul><h3 id=more-lecture-notes>More lecture notes</h3><ul><li><a href=https://github.com/karpathy/nn-zero-to-hero>https://github.com/karpathy/nn-zero-to-hero</a></li><li><a href=https://github.com/Anri-Lombard/micrograd>https://github.com/Anri-Lombard/micrograd</a></li><li><a href=https://github.com/Anri-Lombard/makemore>https://github.com/Anri-Lombard/makemore</a></li></ul><div class="border-t border-b border-gray text-muted my-8 py-4">Read more posts like this in the
<a href=/software-engineering-toolbox>Software Engineering Toolbox</a> collection.</div></div><div class=my-8><a href=https://briansigafoos.com/ class="text-base inline-block mb-1 py-2 px-6 cursor-pointer font-mono font-bold text-primary border border-solid border-primary bg-bright no-underline">Visit homepage</a></div><div class=my-8><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//briansigafoos.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class="border-t border-gray py-4 my-16 w-full text-sm text-faint"><div class="container max-w-4xl text-center font-mono"><div class="sm:flex items-center justify-between"><span>Made with
<a href=https://gohugo.io rel=nofollow target=hugo class=underline>Hugo</a> &
        <a href=https://tailwindcss.com rel=nofollow target=tw class=underline>TailwindCSS</a>
Â·
<a href=https://github.com/BrianSigafoos/hugo_site rel=nofollow target=source class=underline>Source code</a></span>
<span class="block sm:inline mt-2 sm:mt-0">By <a href=/about class="text-primary font-black">Brian Sigafoos</a></span></div></div></div></body></html>