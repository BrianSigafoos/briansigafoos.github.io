<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><title>Learn Neural Networks with Andrej Karpathy - Brian Sigafoos</title><link rel=stylesheet href="/css/main.min.8e029bc338cc175dd7308147e79589660ea30af5f9c43ee44a9263d5d6673335.css" integrity="sha256-jgKbwzjMF13XMIFH55WJZg6jCvX5xD7kSpJj1dZnMzU="><script src=/main.js></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-42740116-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<link rel="shortcut icon" href=/favicon.png?20190809><link rel=apple-touch-icon href=/apple-touch-icon.png?20190809></head><body data-controller=color-scheme class="text-color bg-bright antialiased leading-normal"><div class="border-b border-gray py-4 w-full"><div class="container max-w-4xl"><div class="flex items-center justify-between font-mono"><a href=https://briansigafoos.com/ class="text-primary font-extrabold">Brian Sigafoos</a><div class="flex items-center"><a href=/about class="text-color font-extrabold block">About me</a><div class="flex ml-4"><button type=button data-action=color-scheme#toggleScheme aria-label="Toggle dark mode" title="Toggle dark mode" class="py-1 px-2 cursor-pointer text-color"><svg class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg></button>
<button type=button data-action=color-scheme#toggleColor aria-label="Change primary color" title="Change primary color" class="py-1 px-2 cursor-pointer text-primary"><svg class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zm0 0h12a2 2 0 002-2v-4a2 2 0 00-2-2h-2.343M11 7.343l1.657-1.657a2 2 0 012.828.0l2.829 2.829a2 2 0 010 2.828l-8.486 8.485M7 17h.01"/></svg></button></div></div></div></div></div><div class="container my-12 md:my-16 !max-w-4xl prose md:prose-lg lg:prose-xl prose-custom"><h1>Learn Neural Networks with Andrej Karpathy</h1><p class="mt-4 text-lg md:text-xl lg:text-2xl text-muted">Notes from Karpathy&rsquo;s machine learning lectures - Neural Networks: Zero to Hero</p><div class="font-mono text-sm mt-3 space-x-4"><span class=text-muted>Nov 22, 2022</span>
<span class=text-muted>9 min read</span></div><div class="border-t border-gray my-8 pt-4"><h2 id=intro>Intro</h2><p>These are my notes from the <a href=https://twitter.com/Karpathy>Andrej Karpathy</a> lecture series: <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a>. Andrej is the former Director of AI at Tesla and an excellent teacher. He demystifies complex ML topics like gradient descent through simple examples. When following these videos, I recommend recreating everything Andrej covers on your local machine. It help me practice and build confidence that from simple building blocks we can build up powerful models.</p><p>YouTube playlist: <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a></p><p>YouTube videos:</p><ul><li><ol><li><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to neural networks and backpropagation: building micrograd</a></li></ol></li><li><ol start=2><li><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to language modeling: building makemore</a></li></ol></li><li><ol start=3><li>Coming soon</li></ol></li><li>&mldr;</li></ul><p>Github repos:</p><ul><li><a href=https://github.com/karpathy/micrograd>micrograd</a> - A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API</li><li><a href=https://github.com/karpathy/makemore>makemore</a> - An autoregressive character-level language model for making more things</li><li><a href=https://github.com/karpathy/nn-zero-to-hero>notebooks: nn-zero-to-hero</a> - Lecture notebooks to run locally</li></ul><h3 id=whats-a-neural-network>What&rsquo;s a neural network?</h3><p>Here&rsquo;s how Wikipedia defines a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>:</p><blockquote><p>A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes.</p></blockquote><blockquote><p>Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems.</p></blockquote><blockquote><p>The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination.</p></blockquote><blockquote><p>Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.</p></blockquote><h2 id=setup>Setup</h2><p>First, install Python if you haven&rsquo;t yet.</p><p>Use <a href=https://github.com/pyenv/pyenv-installer>pyenv</a>, similar to rbenv, to easily manage versions.
Note: I recommend an older Python version like 3.9 because PyTorch, a key dependency later on, doesn&rsquo;t always work with the latest version it seems:</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pyenv install -l  <span style=color:#999;font-style:italic># list remote</span>
</span></span><span style=display:flex><span>pyenv install 3.9 <span style=color:#999;font-style:italic># install</span>
</span></span><span style=display:flex><span>pyenv <span style=color:#24909d>local</span> 3.9   <span style=color:#999;font-style:italic># use it locally</span>
</span></span><span style=display:flex><span>pyenv versions    <span style=color:#999;font-style:italic># list local</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pyenv init
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># then update ~/.zshrc with the output to be able to use correct version of</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># python via `python` and `pip` commands without calling `python3`, `pip3`, etc</span>
</span></span></code></pre></div><p>Then, open VS Code and install its <a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python">Python extension</a></p><p>Then <code>git clone</code> Andrej&rsquo;s repositories locally:</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>git clone git@github.com:karpathy/micrograd.git
</span></span><span style=display:flex><span>git clone git@github.com:karpathy/makemore.git
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Open in VS Code</span>
</span></span><span style=display:flex><span>code micrograd
</span></span></code></pre></div><p>Open a Jupyter Notebook <code>.ipynb</code> file in <code>micrograd</code> and select the <code>pyenv</code> version you installed, plus install any VSCode recommended extensions for Jupyter Notebooks.</p><p>Create a new file called <code>youtube1.ipynb</code> or something similar so you can run same commands that Andrej does during his videos.</p><p>Now you&rsquo;re all set to dive into the videos.</p><h2 id=1-micrograd>1. Micrograd</h2><p>Watch the YouTube video: <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to neural networks and backpropagation: building micrograd</a>.</p><p>Code source: <a href=https://github.com/karpathy/micrograd>micrograd</a></p><p>And follow along locally in your own <code>.ipynb</code> file. I strongly recommend doing this yourself, typing out everything that Andrej does, and running it all locally. This &ldquo;practice&rdquo; helps me learn the content better and builds confidence that it can all be recreated locally.</p><p>Just for reference (create your own!), here are examples of local notebooks:</p><ul><li>My notebook from the <a href=https://github.com/BrianSigafoos/micrograd/blob/master/micrograd_youtube1.ipynb>1st part of the micrograd video</a>.</li><li>Two more complete <a href=https://github.com/Anri-Lombard/micrograd/tree/main/Lectures>notebooks for both parts of the micrograd video</a> by Github user @Anri-Lombard.</li></ul><p>Everything needed to understand a neural network is in <a href=https://github.com/karpathy/micrograd>micrograd</a>. Everything else is just efficiency.
There are only 150 lines of code in <code>micrograd/engine.py</code> and <code>micrograd/nn.py</code>.</p><p>&ldquo;Back propagation is recursive application of chain rule, backwards through the computation graph.&rdquo;</p><p>If can write local gradients and can do backward propagation of gradients, then it doesn&rsquo;t matter if it&rsquo;s a compound function or separate functions (equal to compound function), the result will be the same.</p><p><code>micrograd</code> is for scalar values (1.0, etc)
PyTorch is for a tensor, an n-dimensional array of scalars</p><p>Gradient descent is the iteration of:</p><ul><li>forward pass</li><li>backward pass</li><li>update</li></ul><p>The neural net improved its predictions with each iteration.</p><p>Most common neural net mistakes from <a href=https://twitter.com/karpathy/status/1013244313327681536>@karpathy&rsquo;s tweet</a>:</p><ol><li>you didn&rsquo;t try to overfit a single batch first.</li><li>you forgot to toggle train/eval mode for the net.</li><li>you forgot to .zero_grad() (in pytorch) before .backward().</li><li>you passed softmaxed outputs to a loss that expects raw logits.</li><li>you didn&rsquo;t use bias=False for your Linear/Conv2d layer when using BatchNorm, or conversely forget to include it for the output layer. This one won&rsquo;t make you silently fail, but they are spurious parameters</li><li>thinking view() and permute() are the same thing (& incorrectly using view)</li></ol><blockquote class=twitter-tweet><p lang=en dir=ltr>most common neural net mistakes: 1) you didn't try to overfit a single batch first. 2) you forgot to toggle train/eval mode for the net. 3) you forgot to .zero_grad() (in pytorch) before .backward(). 4) you passed softmaxed outputs to a loss that expects raw logits. ; others? :)</p>&mdash; Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/1013244313327681536?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><h2 id=2-makemore---bigram-character-level-language-model>2. Makemore - bigram character-level language model</h2><p>Watch the YouTube video: <a href="https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to language modeling: building makemore</a>.</p><p>Code source: <a href=https://github.com/karpathy/makemore>makemore</a></p><p>Every line in <code>makemore/names.txt</code> is an example. Each example is a sequence of characters.
We&rsquo;re building a character level language model. It knows how to predict the next character in the sequence.</p><p>As noted in the video, it&rsquo;s important to learn more about <a href="https://pytorch.org/docs/stable/notes/broadcasting.html?highlight=broadcasting">Broadcasting semantics</a>.</p><p>This lecture let&rsquo;s us train a bigram language model.</p><h3 id=setup-code>Setup code</h3><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Setup - common to both approaches</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>torch</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># data set: 32k first names</span>
</span></span><span style=display:flex><span>words = <span style=color:#24909d>open</span>(<span style=color:#ed9d13>&#39;names.txt&#39;</span>, <span style=color:#ed9d13>&#39;r&#39;</span>).read().splitlines()
</span></span><span style=display:flex><span>chars = <span style=color:#24909d>sorted</span>(<span style=color:#24909d>list</span>(<span style=color:#24909d>set</span>(<span style=color:#ed9d13>&#39;&#39;</span>.join(words))))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># s to i lookup, setting `.` as 0 index in array and all others + 1</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># we&#39;ll use `.` to mark the start and end of all words</span>
</span></span><span style=display:flex><span>stoi = {s: i+<span style=color:#3677a9>1</span> <span style=color:#6ab825;font-weight:700>for</span> i, s <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(chars)}
</span></span><span style=display:flex><span>stoi[<span style=color:#ed9d13>&#39;.&#39;</span>] = <span style=color:#3677a9>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># i to s lookup</span>
</span></span><span style=display:flex><span>itos = {i: s <span style=color:#6ab825;font-weight:700>for</span> s, i <span style=color:#6ab825;font-weight:700>in</span> stoi.items()}
</span></span></code></pre></div><h3 id=non-neural-network-approach>Non-neural network approach</h3><p>We start training it by counting how frequently any pairing of letters occurs in ~32k names, and then normalizing so we get a nice probability distribution.</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Approach 1: non-neural network approach: count frequency of bigrams and store in `N`</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Create a 27x27 matrix with values all set to 0</span>
</span></span><span style=display:flex><span>N = torch.zeros((<span style=color:#3677a9>27</span>, <span style=color:#3677a9>27</span>), dtype=torch.int32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Get the counts</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> w <span style=color:#6ab825;font-weight:700>in</span> words:
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># use `.` to mark the start and end of all words</span>
</span></span><span style=display:flex><span>  chs = [<span style=color:#ed9d13>&#39;.&#39;</span>] + <span style=color:#24909d>list</span>(w) + [<span style=color:#ed9d13>&#39;.&#39;</span>]
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>for</span> ch1, ch2 <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>zip</span>(chs, chs[<span style=color:#3677a9>1</span>:]):
</span></span><span style=display:flex><span>    <span style=color:#999;font-style:italic># integer index of this character in stoi 0-27</span>
</span></span><span style=display:flex><span>    ix1 = stoi[ch1]
</span></span><span style=display:flex><span>    ix2 = stoi[ch2]
</span></span><span style=display:flex><span>    N[ix1, ix2] += <span style=color:#3677a9>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># prepare probabilities, parameters of our bigram language model</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Apply &#34;model smoothing&#34; using `N + 1` instead of `N`. This prevents 0&#39;s in probability matrix P, which could lead to `infinity` for loss measurement.</span>
</span></span><span style=display:flex><span>P = (N + <span style=color:#3677a9>1</span>).float()
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># 27, 27</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># 27, 1  # This is &#34;broadcastable&#34; and it stretches the 1 into all 27 rows</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># https://pytorch.org/docs/stable/notes/broadcasting.html?highlight=broadcasting</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Below uses `/=` to avoid creating new tensor, ie more efficient</span>
</span></span><span style=display:flex><span>P /= P.sum(<span style=color:#3677a9>1</span>, keepdim=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g = torch.Generator().manual_seed(<span style=color:#3677a9>2147483647</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Sample</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>5</span>):
</span></span><span style=display:flex><span>  out = []
</span></span><span style=display:flex><span>  ix = <span style=color:#3677a9>0</span>
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>while</span> <span style=color:#6ab825;font-weight:700>True</span>:
</span></span><span style=display:flex><span>    p = P[ix]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ix = torch.multinomial(p, num_samples=<span style=color:#3677a9>1</span>, replacement=<span style=color:#6ab825;font-weight:700>True</span>, generator=g).item()
</span></span><span style=display:flex><span>    out.append(itos[ix])
</span></span><span style=display:flex><span>    <span style=color:#999;font-style:italic># Break with `.` is found, marking the end of the word</span>
</span></span><span style=display:flex><span>    <span style=color:#6ab825;font-weight:700>if</span> ix == <span style=color:#3677a9>0</span>:
</span></span><span style=display:flex><span>      <span style=color:#6ab825;font-weight:700>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#24909d>print</span>(<span style=color:#ed9d13>&#39;&#39;</span>.join(out))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Output:</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   mor.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   axx.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   minaymoryles.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   kondlaisah.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   anchshizarie.</span>
</span></span></code></pre></div><h3 id=loss-function>Loss function</h3><p>Then we can evaluate the quality of this model.</p><p>Goal: summarize probabilities into a single number that measure the quality of this model.</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Goal: summarize probabilities into a single number that measure the quality of this model</span>
</span></span><span style=display:flex><span>log_likelihood = <span style=color:#3677a9>0.0</span>
</span></span><span style=display:flex><span>n = <span style=color:#3677a9>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> w <span style=color:#6ab825;font-weight:700>in</span> words:
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># for w in [&#34;andrejq&#34;]:</span>
</span></span><span style=display:flex><span>  chs = [<span style=color:#ed9d13>&#39;.&#39;</span>] + <span style=color:#24909d>list</span>(w) + [<span style=color:#ed9d13>&#39;.&#39;</span>]
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>for</span> ch1, ch2 <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>zip</span>(chs, chs[<span style=color:#3677a9>1</span>:]):
</span></span><span style=display:flex><span>    ix1 = stoi[ch1]
</span></span><span style=display:flex><span>    ix2 = stoi[ch2]
</span></span><span style=display:flex><span>    prob = P[ix1, ix2]
</span></span><span style=display:flex><span>    logprob = torch.log(prob)
</span></span><span style=display:flex><span>    <span style=color:#999;font-style:italic># This is because: log(a*b*c) = log(a) + log(b) + log(c)</span>
</span></span><span style=display:flex><span>    log_likelihood += logprob
</span></span><span style=display:flex><span>    n += <span style=color:#3677a9>1</span>
</span></span><span style=display:flex><span>    <span style=color:#24909d>print</span>(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>{</span>ch1<span style=color:#ed9d13>}{</span>ch2<span style=color:#ed9d13>}</span><span style=color:#ed9d13>: </span><span style=color:#ed9d13>{</span>prob<span style=color:#ed9d13>:</span><span style=color:#ed9d13>.4f</span><span style=color:#ed9d13>}</span><span style=color:#ed9d13> </span><span style=color:#ed9d13>{</span>logprob<span style=color:#ed9d13>:</span><span style=color:#ed9d13>.4f</span><span style=color:#ed9d13>}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>{</span>log_likelihood<span style=color:#ed9d13>=}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># negative log likelihood is a nice loss function.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># The lowest it can get is 0. The higher it is, the worse off the predictions</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># you are making are.</span>
</span></span><span style=display:flex><span>nll = -log_likelihood
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>{</span>nll<span style=color:#ed9d13>=}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Above was the sum negative log likelihood. Better is the average negative log likelihood.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># So divide that sum by `n` to get the average.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># So the loss function for the training set assigned by this model is 2.4. That&#39;s the &#34;quality&#34; of this model.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># The lower it is the better off we are. The higher it is the worse off we are.</span>
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>{</span>nll/n<span style=color:#ed9d13>}</span><span style=color:#ed9d13>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Output:</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   log_likelihood=tensor(-559951.5625)</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   nll=tensor(559951.5625)</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#   2.4543561935424805</span>
</span></span></code></pre></div><h3 id=neural-network-approach>Neural network approach</h3><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Approach 2: neural network approach trained on bigrams</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># for one hot encoding: `F.one_hot` below</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>torch.nn.functional</span> <span style=color:#6ab825;font-weight:700>as</span> <span style=color:#447fcf;text-decoration:underline>F</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Dataset: 228K bigrams from the 32K example names</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic>#</span>
</span></span><span style=display:flex><span>xs, ys = [], []
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> w <span style=color:#6ab825;font-weight:700>in</span> words:
</span></span><span style=display:flex><span>  chs = [<span style=color:#ed9d13>&#39;.&#39;</span>] + <span style=color:#24909d>list</span>(w) + [<span style=color:#ed9d13>&#39;.&#39;</span>]
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>for</span> ch1, ch2 <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>zip</span>(chs, chs[<span style=color:#3677a9>1</span>:]):
</span></span><span style=display:flex><span>    ix1 = stoi[ch1]
</span></span><span style=display:flex><span>    ix2 = stoi[ch2]
</span></span><span style=display:flex><span>    xs.append(ix1)
</span></span><span style=display:flex><span>    ys.append(ix2)
</span></span><span style=display:flex><span>xs = torch.tensor(xs)
</span></span><span style=display:flex><span>ys = torch.tensor(ys)
</span></span><span style=display:flex><span>num = xs.nelement()
</span></span><span style=display:flex><span><span style=color:#24909d>print</span>(<span style=color:#ed9d13>&#39;number of examples: &#39;</span>, num)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># initialize the &#39;network&#39;</span>
</span></span><span style=display:flex><span>g = torch.Generator().manual_seed(<span style=color:#3677a9>2147483647</span>)
</span></span><span style=display:flex><span>W = torch.randn((<span style=color:#3677a9>27</span>, <span style=color:#3677a9>27</span>), generator=g, requires_grad=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span></code></pre></div><p>Gradient descent</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Gradient descent</span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> k <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>100</span>):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># forward pass</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># input to the network: one-hot encoding</span>
</span></span><span style=display:flex><span>  xenc = F.one_hot(xs, num_classes=<span style=color:#3677a9>27</span>).float()
</span></span><span style=display:flex><span>  logits = xenc @ W  <span style=color:#999;font-style:italic># predict log-counts</span>
</span></span><span style=display:flex><span>  counts = logits.exp()  <span style=color:#999;font-style:italic># counts, equivalent to N</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># probabilities for next character</span>
</span></span><span style=display:flex><span>  probs = counts / counts.sum(<span style=color:#3677a9>1</span>, keepdims=<span style=color:#6ab825;font-weight:700>True</span>)
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># regularization loss: `0.01*(W**2).mean()` tries to make all W&#39;s 0</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># if `0.01` is higher it will be more uniform and not</span>
</span></span><span style=display:flex><span>  loss = -probs[torch.arange(num), ys].log().mean() + <span style=color:#3677a9>0.01</span> * (W**<span style=color:#3677a9>2</span>).mean()
</span></span><span style=display:flex><span>  <span style=color:#24909d>print</span>(loss.item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># backward pass</span>
</span></span><span style=display:flex><span>  W.grad = <span style=color:#6ab825;font-weight:700>None</span>  <span style=color:#999;font-style:italic># set to zero the gradient</span>
</span></span><span style=display:flex><span>  loss.backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># update</span>
</span></span><span style=display:flex><span>  W.data += -<span style=color:#3677a9>50</span> * W.grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># Earlier we had 2.47 loss when we manually did the counts.</span>
</span></span><span style=display:flex><span><span style=color:#999;font-style:italic># So we&#39;d like this neural network approach to become as &#34;good&#34;, when measuring the loss.</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999;font-style:italic># Sample from neural net model</span>
</span></span><span style=display:flex><span>g = torch.Generator().manual_seed(<span style=color:#3677a9>2147483647</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>5</span>):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  out = []
</span></span><span style=display:flex><span>  ix = <span style=color:#3677a9>0</span>
</span></span><span style=display:flex><span>  <span style=color:#6ab825;font-weight:700>while</span> <span style=color:#6ab825;font-weight:700>True</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    xenc = F.one_hot(torch.tensor([ix]), num_classes=<span style=color:#3677a9>27</span>).float()
</span></span><span style=display:flex><span>    logits = xenc @ W  <span style=color:#999;font-style:italic># predict log-counts</span>
</span></span><span style=display:flex><span>    counts = logits.exp()  <span style=color:#999;font-style:italic># counts, equivalent to N</span>
</span></span><span style=display:flex><span>    p = counts / counts.sum(<span style=color:#3677a9>1</span>, keepdims=<span style=color:#6ab825;font-weight:700>True</span>) <span style=color:#999;font-style:italic># probabilities for next character</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ix = torch.multinomial(p, num_samples=<span style=color:#3677a9>1</span>, replacement=<span style=color:#6ab825;font-weight:700>True</span>, generator=g).item()
</span></span><span style=display:flex><span>    out.append(itos[ix])
</span></span><span style=display:flex><span>    <span style=color:#6ab825;font-weight:700>if</span> ix == <span style=color:#3677a9>0</span>:
</span></span><span style=display:flex><span>      <span style=color:#6ab825;font-weight:700>break</span>
</span></span><span style=display:flex><span>  <span style=color:#24909d>print</span>(<span style=color:#ed9d13>&#39;&#39;</span>.join(out))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic># Output:  almost identical to approach 1 non-neural network with count frequencies</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   mor.</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   axx.</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   minaymoryles.</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   kondlaisah.</span>
</span></span><span style=display:flex><span>  <span style=color:#999;font-style:italic>#   anchshizarie.</span>
</span></span></code></pre></div><h2 id=3->3. &mldr;</h2><h2 id=references>References</h2><p>YouTube playlist: <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a></p><p>YouTube videos:</p><ul><li><ol><li><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to neural networks and backpropagation: building micrograd</a></li></ol></li><li><ol start=2><li><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to language modeling: building makemore</a></li></ol></li><li>&mldr;</li></ul><p>Github repos:</p><ul><li><a href=https://github.com/karpathy/micrograd>micrograd</a> - A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API</li><li><a href=https://github.com/karpathy/makemore>makemore</a> - An autoregressive character-level language model for making more things</li></ul><h3 id=more-lecture-notes>More lecture notes</h3><ul><li><a href=https://github.com/karpathy/nn-zero-to-hero>https://github.com/karpathy/nn-zero-to-hero</a></li><li><a href=https://github.com/Anri-Lombard/micrograd>https://github.com/Anri-Lombard/micrograd</a></li><li><a href=https://github.com/Anri-Lombard/makemore>https://github.com/Anri-Lombard/makemore</a></li></ul><div class="border-t border-b border-gray text-muted my-8 py-4">Read more posts like this in the
<a href=/software-engineering-toolbox>Software Engineering Toolbox</a> collection.</div></div><div class=my-8><a href=https://briansigafoos.com/ class="text-base inline-block mb-1 py-2 px-6 cursor-pointer font-mono font-bold text-primary border border-solid border-primary bg-bright no-underline">Visit homepage</a></div><div class=my-8><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//briansigafoos.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class="border-t border-gray py-4 my-16 w-full text-sm text-faint"><div class="container max-w-4xl text-center font-mono"><div class="sm:flex items-center justify-between"><span>Made with
<a href=https://gohugo.io rel=nofollow target=hugo class=underline>Hugo</a> &
        <a href=https://tailwindcss.com rel=nofollow target=tw class=underline>TailwindCSS</a>
·
<a href=https://github.com/BrianSigafoos/hugo_site rel=nofollow target=source class=underline>Source code</a></span>
<span class="block sm:inline mt-2 sm:mt-0">By <a href=/about class="text-primary font-black">Brian Sigafoos</a></span></div></div></div></body></html>